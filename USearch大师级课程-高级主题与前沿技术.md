# ğŸ”¥ USearchå¤§å¸ˆçº§è¯¾ç¨‹ - é«˜çº§ä¸»é¢˜ä¸å‰æ²¿æŠ€æœ¯

> æ·±å…¥æ¢ç´¢USearchæœªè¢«è¦†ç›–çš„é«˜çº§ä¸»é¢˜:ä»æ±‡ç¼–çº§ä¼˜åŒ–åˆ°åˆ†å¸ƒå¼æ¶æ„

**ç‰ˆæœ¬:** v1.0 | **éš¾åº¦:** â­â­â­â­â­ | **å‰ç½®è¦æ±‚:** å®Œæˆ14å¤©æ·±åº¦è¿›é˜¶è¯¾ç¨‹

---

## ğŸ“š è¯¾ç¨‹å¯¼èˆª

### ç¬¬ä¸€éƒ¨åˆ†: åº•å±‚ç³»ç»Ÿä¼˜åŒ– (Advanced 1-5)
- **Advanced 1:** æ±‡ç¼–çº§æ€§èƒ½åˆ†æä¸å†…è”æ±‡ç¼–
- **Advanced 2:** CPUäº²å’Œæ€§ä¸NUMAä¼˜åŒ–
- **Advanced 3:** JITç¼–è¯‘ä¸åŠ¨æ€ä»£ç ç”Ÿæˆ
- **Advanced 4:** TLBä¼˜åŒ–ä¸å¤§é¡µå†…å­˜
- **Advanced 5:** ç¡¬ä»¶æ€§èƒ½è®¡æ•°å™¨æ·±åº¦åº”ç”¨

### ç¬¬äºŒéƒ¨åˆ†: ç®—æ³•å‰æ²¿ (Advanced 6-9)
- **Advanced 6:** Product Quantizationå®ç°ç»†èŠ‚
- **Advanced 7:** å›¾ç¥ç»ç½‘ç»œä¸HNSWèåˆ
- **Advanced 8:** è‡ªé€‚åº”å‚æ•°è°ƒä¼˜ç®—æ³•
- **Advanced 9:** åœ¨çº¿ç´¢å¼•é‡å¹³è¡¡ä¸è¿ç§»

### ç¬¬ä¸‰éƒ¨åˆ†: åˆ†å¸ƒå¼æ¶æ„ (Advanced 10-12)
- **Advanced 10:** åˆ†å¸ƒå¼HNSWè®¾è®¡ä¸å®ç°
- **Advanced 11:** ä¸€è‡´æ€§å“ˆå¸Œä¸æ•°æ®åˆ†ç‰‡
- **Advanced 12:** Raftåè®®ä¸ç´¢å¼•å¤åˆ¶

### ç¬¬å››éƒ¨åˆ†: å¼‚æ„è®¡ç®— (Advanced 13-15)
- **Advanced 13:** CUDAåŠ é€Ÿå‘é‡æœç´¢
- **Advanced 14:** AVX-512æ·±åº¦ä¼˜åŒ–æŠ€å·§
- **Advanced 15:** FPGAåŠ é€Ÿä¸HLSè®¾è®¡

---

## Advanced 1: æ±‡ç¼–çº§æ€§èƒ½åˆ†æä¸å†…è”æ±‡ç¼– ğŸ”§

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦æ·±å…¥æ±‡ç¼–?

**åœºæ™¯:** ä½ å·²ç»åº”ç”¨äº†æ‰€æœ‰C++ä¼˜åŒ–æŠ€å·§,ä½†æ€§èƒ½ä»æœªè¾¾åˆ°é¢„æœŸ

```cpp
// ç¤ºä¾‹: ä½™å¼¦è·ç¦»è®¡ç®—
float cosine(const float* a, const float* b, size_t n) {
    float dot = 0, norm_a = 0, norm_b = 0;
    for (size_t i = 0; i < n; i++) {
        dot += a[i] * b[i];
        norm_a += a[i] * a[i];
        norm_b += b[i] * b[i];
    }
    return dot / (sqrtf(norm_a) * sqrtf(norm_b));
}
```

**é—®é¢˜:** å³ä½¿å¼€å¯`-O3`ä¼˜åŒ–,æ€§èƒ½ä»ä¸å¦‚é¢„æœŸ

**è§£å†³:** å¿…é¡»æŸ¥çœ‹ç¼–è¯‘å™¨ç”Ÿæˆçš„æ±‡ç¼–ä»£ç 

### 1.2 æ±‡ç¼–åˆ†æå·¥å…·é“¾

#### å·¥å…·1: Compiler Explorer (Godbolt)

```bash
# è®¿é—® https://godbolt.org/

# é…ç½®:
# - Compiler: GCC 13.2
# - Options: -O3 -march=skylake -RSpace=asm
# - å°†ä¸Šè¿°cosineå‡½æ•°ç²˜è´´

# è§‚å¯Ÿ:
1. å¾ªç¯æ˜¯å¦å‘é‡åŒ–?
2. æ˜¯å¦ä½¿ç”¨FMAæŒ‡ä»¤?
3. æ˜¯å¦æœ‰ä¸å¿…è¦çš„å†…å­˜è®¿é—®?
```

**å®é™…æ±‡ç¼–è¾“å‡ºåˆ†æ:**

```asm
# GCC -O3 -march=skylake
cosine(float const*, float const*, unsigned long):
    test    rdx, rdx              # æ£€æŸ¥næ˜¯å¦ä¸º0
    je      .L10                  # ä¸º0åˆ™è·³è½¬
    xorps   xmm0, xmm0            # dot = 0
    xorps   xmm1, xmm1            # norm_a = 0
    xorps   xmm2, xmm2            # norm_b = 0
    xor     eax, eax              # i = 0
.L3:
    movss   xmm3, DWORD PTR [rdi+rax*4]  # åŠ è½½a[i]
    movss   xmm4, DWORD PTR [rsi+rax*4]  # åŠ è½½b[i]
    movaps  xmm5, xmm3                     # æ‹·è´a[i]
    mulss   xmm5, xmm4                     # a[i] * b[i]
    addss   xmm0, xmm5                     # dot += ...
    mulss   xmm3, xmm3                     # a[i] * a[i]
    addss   xmm1, xmm3                     # norm_a += ...
    mulss   xmm4, xmm4                     # b[i] * b[i]
    addss   xmm2, xmm4                     # norm_b += ...
    add     rax, 1                         # i++
    cmp     rax, rdx                       # i < n?
    jne     .L3                           # ç»§ç»­å¾ªç¯

    # é—®é¢˜: æ¯æ¬¡è¿­ä»£åªå¤„ç†ä¸€ä¸ªå…ƒç´ !æœªå‘é‡åŒ–!
    sqrtss  xmm1, xmm1             # sqrt(norm_a)
    sqrtss  xmm2, xmm2             # sqrt(norm_b)
    mulss   xmm1, xmm2             # sqrt(norm_a) * sqrt(norm_b)
    divss   xmm0, xmm1             # dot / product
    ret
```

**è¯Šæ–­:** ç¼–è¯‘å™¨æ²¡æœ‰å‘é‡åŒ–,å› ä¸º:
1. å¾ªç¯ä½“å†…æœ‰å¤šä¸ªç‹¬ç«‹çš„ç´¯åŠ å™¨
2. å­˜åœ¨æ•°æ®ä¾èµ–å…³ç³»

#### å·¥å…·2: objdumpåæ±‡ç¼–

```bash
# ç¼–è¯‘æ—¶ç”Ÿæˆæ±‡ç¼–
g++ -O3 -march=native -S -c cosine.cpp -o cosine.s

# æˆ–åç¼–è¯‘ç›®æ ‡æ–‡ä»¶
g++ -O3 -march=native -c cosine.cpp -o cosine.o
objdump -d cosine.o | less

# å¸¦æºç æ³¨é‡Šçš„åæ±‡ç¼–
g++ -O3 -march=native -g -c cosine.cpp -o cosine.o
objdump -d -S cosine.o | less
```

#### å·¥å…·3: perf annotate

```bash
# è®°å½•æ€§èƒ½æ•°æ®
perf record -g ./your_program

# æ³¨é‡Šæ±‡ç¼–ä»£ç 
perf annotate cosine

# è¾“å‡ºç¤ºä¾‹:
#   35.50 â”‚    movss   xmm3, DWORD PTR [rdi+rax*4]
#   12.30 â”‚    movss   xmm4, DWORD PTR [rsi+rax*4]
#         â”‚    movaps  xmm5, xmm3
#   18.20 â”‚    mulss   xmm5, xmm4
#   â”‚      addss   xmm0, xmm5
# ...
# ç™¾åˆ†æ¯”è¡¨ç¤ºåœ¨è¯¥æŒ‡ä»¤èŠ±è´¹çš„æ—¶é—´
```

### 1.3 å¼ºåˆ¶å‘é‡åŒ–ä¼˜åŒ–

**æ–¹æ³•1: OpenMP SIMD**

```cpp
// æ·»åŠ OpenMP SIMDæŒ‡ä»¤
#pragma omp declare simd
float cosine(const float* a, const float* b, size_t n) {
    float dot = 0, norm_a = 0, norm_b = 0;
    #pragma omp simd reduction(+:dot,norm_a,norm_b)
    for (size_t i = 0; i < n; i++) {
        dot += a[i] * b[i];
        norm_a += a[i] * a[i];
        norm_b += b[i] * b[i];
    }
    return dot / (sqrtf(norm_a) * sqrtf(norm_b));
}
```

**ç¼–è¯‘:**
```bash
g++ -O3 -march=native -fopenmp -fopenmp-simd cosine.cpp
```

**ç”Ÿæˆçš„æ±‡ç¼– (AVX2, 8ä¸ªfloatå¹¶è¡Œ):**

```asm
cosine:
    # ... åˆå§‹åŒ– ...
    vxorps  ymm0, ymm0, ymm0       # 8ä¸ªdotåˆå§‹å€¼
    vxorps  ymm1, ymm1, ymm1       # 8ä¸ªnorm_a
    vxorps  ymm2, ymm2, ymm2       # 8ä¸ªnorm_b
    xor     eax, eax
    mov     rcx, rdx
    shr     rcx, 3                 # å¾ªç¯æ¬¡æ•° = n / 8
    test    rcx, rcx
    je      .L_end_loop
.L_loop:
    # åŠ è½½8ä¸ªfloat
    vmovups ymm3, YMMWORD PTR [rdi+rax*4]
    vmovups ymm4, YMMWORD PTR [rsi+rax*4]

    # FMA: a[i] * b[i] + dot
    vfmadd231ps ymm0, ymm3, ymm4   # dot += a * b
    vfmadd231ps ymm1, ymm3, ymm3   # norm_a += a * a
    vfmadd231ps ymm2, ymm4, ymm4   # norm_b += b * b

    add     rax, 8
    dec     rcx
    jne     .L_loop

    # æ°´å¹³æ±‚å’ŒYMMå¯„å­˜å™¨
    vextractf128 xmm3, ymm0, 1     # é«˜128ä½
    vaddps  xmm0, xmm0, xmm3       # low + high
    vhaddps xmm0, xmm0, xmm0       # æ°´å¹³åŠ æ³•
    # ... å¯¹norm_a, norm_bé‡å¤ ...
```

**æ€§èƒ½æå‡:** 8å€ (256ä½ / 32ä½)

**æ–¹æ³•2: GCCå‘é‡åŒ–æç¤º**

```cpp
float cosine(const float* __restrict__ a,
             const float* __restrict__ b,
             size_t n) {
    float dot = 0, norm_a = 0, norm_b = 0;

    // å‘Šè¯‰ç¼–è¯‘å™¨å¾ªç¯æ¬¡æ•°>=16,ä¸”å‘é‡å¤§å°ä¸º8
    for (size_t i = 0; i < n; i++) {
        dot += a[i] * b[i];
        norm_a += a[i] * a[i];
        norm_b += b[i] * b[i];
    }

    return dot / (sqrtf(norm_a) * sqrtf(norm_b));
}
```

### 1.4 å†…è”æ±‡ç¼–ä¼˜åŒ–

**åœºæ™¯:** éœ€è¦ä½¿ç”¨ç‰¹å®šCPUæŒ‡ä»¤,ä½†ç¼–è¯‘å™¨ä¸æ”¯æŒ

**ç¤ºä¾‹: AVX-512 BW (Byte and Word)æŒ‡ä»¤**

```cpp
#include <immintrin.h>

// ä½¿ç”¨AVX-512 VNNI (Vector Neural Network Instructions)
// æ··åˆç²¾åº¦çŸ©é˜µä¹˜æ³•ä¼˜åŒ–
void avx512_vnni_matmul(
    const int8_t* A,  // 8-bitæ•´æ•°çŸ©é˜µ
    const int8_t* B,
    int32_t* C,
    int M, int N, int K) {

    #pragma omp parallel for collapse(2)
    for (int m = 0; m < M; m++) {
        for (int n = 0; n < N; n += 16) {  // AVX-512å¤„ç†16ä¸ªint32

            __m512i sum = _mm512_setzero_si512();

            for (int k = 0; k < K; k += 64) {  // 512ä½ / 8ä½ = 64

                // åŠ è½½64ä¸ªint8_t
                __m512i a_vec = _mm512_loadu_si512((__m512i*)(A + m*K + k));

                // å†…è”æ±‡ç¼–: ä½¿ç”¨VPDPBUSDæŒ‡ä»¤
                // VNNIæŒ‡ä»¤: 8-bitä¹˜æ³• + 32-bitç´¯åŠ 
                __m512i b_vec = _mm512_loadu_si512((__m512i*)(B + k*N + n));

                asm volatile (
                    "vpdpbusd %2, %1, %0"  // FMA: int8 * int8 + int32
                    : "+v"(sum)
                    : "v"(a_vec), "v"(b_vec)
                );
            }

            // å­˜å‚¨ç»“æœ
            _mm512_storeu_si512((__m512i*)(C + m*N + n), sum);
        }
    }
}
```

**æ€§èƒ½:** æ¯”æ ‡å‡†int32ä¹˜æ³•å¿«**16å€**

### 1.5 æ‰‹å†™æ±‡ç¼–ä¼˜åŒ–å®ä¾‹

**ç›®æ ‡:** ä¼˜åŒ–USearchä¸­çš„è·ç¦»è®¡ç®—

**æºç ä½ç½®:** `index_dense.hpp:1567-1589`

```cpp
// USearchåŸå§‹å®ç° (å·²ä¼˜åŒ–)
template <typename metric_at, typename scalar_at>
struct metric_state_gt<metric_at, scalar_at, metric_kind_t::cos_k> {
    using value_type = scalar_at;

    template <typename value_at>
    inline void operator()(value_at const* a, value_at const* b, std::size_t n) {
        value_type dot = 0, norm_a = 0, norm_b = 0;

        // éšå¼å‘é‡åŒ–å¾ªç¯
        for (std::size_t i = 0; i != n; ++i) {
            value_type ai = a[i];
            value_type bi = b[i];
            dot += ai * bi;
            norm_a += ai * ai;
            norm_b += bi * bi;
        }

        return value_type(1) - dot / sqrtf(norm_a * norm_b);
    }
};
```

**æ‰‹å†™æ±‡ç¼–ç‰ˆæœ¬ (AVX-512):**

```asm
# input:  rdi = a*, rsi = b*, rdx = n
# output: xmm0 = cosine distance
# clobbered: ymm0-ymm15, rax, rcx

cosine_avx512:
    # æ£€æŸ¥å‘é‡å¤§å°
    test    rdx, rdx
    je      .L_return

    # åˆå§‹åŒ–ç´¯åŠ å™¨ (ä½¿ç”¨å¹¿æ’­)
    vpxor   ymm0, ymm0, ymm0          # dot = 0
    vpxor   ymm1, ymm1, ymm1          # norm_a = 0
    vpxor   ymm2, ymm2, ymm2          # norm_b = 0

    # å¤„ç†64å­—èŠ‚å— (16ä¸ªfloat)
    mov     rcx, rdx
    shr     rcx, 4                    # n / 16
    je      .L_skip_64
.L_loop_64:
    # é¢„å–ä¸‹ä¸€ä¸ªå—
    prefetcht0 [rdi + 256]
    prefetcht0 [rsi + 256]

    # åŠ è½½16ä¸ªfloat
    vmovups zmm3, ZMMWORD PTR [rdi]
    vmovups zmm4, ZMMWORD PTR [rsi]

    # FMAæŒ‡ä»¤ (3ä¸ªæ“ä½œæ•°)
    vfmadd231ps zmm0, zmm3, zmm4      # dot += a * b
    vfmadd231ps zmm1, zmm3, zmm3      # norm_a += a * a
    vfmadd231ps zmm2, zmm4, zmm4      # norm_b += b * b

    add     rdi, 64
    add     rsi, 64
    dec     rcx
    jne     .L_loop_64

.L_skip_64:
    # å¤„ç†32å­—èŠ‚å— (8ä¸ªfloat, AVX2)
    mov     rcx, rdx
    shr     rcx, 3
    and     rcx, 1                    # åªå¤„ç†å‰©ä½™1æ¬¡
    je      .L_skip_32
    vmovups ymm3, YMMWORD PTR [rdi]
    vmovups ymm4, YMMWORD PTR [rsi]
    vfmadd231ps ymm0, ymm3, ymm4
    vfmadd231ps ymm1, ymm3, ymm3
    vfmadd231ps ymm2, ymm4, ymm4
    add     rdi, 32
    add     rsi, 32

.L_skip_32:
    # å¤„ç†æ ‡é‡å‰©ä½™éƒ¨åˆ†
    mov     rcx, rdx
    and     rcx, 7
    je      .L_finalize
.L_scalar:
    movss   xmm3, DWORD PTR [rdi]
    movss   xmm4, DWORD PTR [rsi]
    vfmadd231ss xmm0, xmm3, xmm4
    vfmadd231ss xmm1, xmm3, xmm3
    vfmadd231ss xmm2, xmm4, xmm4
    add     rdi, 4
    add     rsi, 4
    dec     rcx
    jne     .L_scalar

.L_finalize:
    # æ°´å¹³å½’çº¦ZMMå¯„å­˜å™¨ (16ä¸ªfloat -> 1ä¸ª)
    vextractf64x4 ymm3, zmm0, 1      # é«˜åŠéƒ¨åˆ†
    vaddps  ymm0, ymm0, ymm3          # åˆå¹¶
    vextractf128 xmm3, ymm0, 1       # é«˜128ä½
    vaddps  xmm0, xmm0, xmm3
    vhaddps xmm0, xmm0, xmm0
    vhaddps xmm0, xmm0, xmm0
    # å¯¹norm_aå’Œnorm_bé‡å¤...

    # è®¡ç®— sqrt(norm_a * norm_b)
    vmulss  xmm1, xmm1, xmm2          # norm_a * norm_b
    vsqrtss xmm1, xmm1, xmm1          # sqrt(...)
    vdivss  xmm0, xmm0, xmm1          # dot / sqrt(...)

    # è½¬æ¢ä¸ºè·ç¦»: 1 - similarity
    movss   xmm1, _LTOW_1_0_         # åŠ è½½å¸¸é‡1.0
    vsubss  xmm0, xmm1, xmm0          # 1 - dot / sqrt(...)

.L_return:
    ret
```

**æ€§èƒ½å¯¹æ¯”:**

| å®ç° | 768ç»´ä½™å¼¦è·ç¦» | ç›¸å¯¹æ€§èƒ½ |
|------|--------------|---------|
| æ ‡é‡ | 245 ns | 1.0x |
| AVX2 | 52 ns | 4.7x |
| AVX-512 | 28 ns | **8.8x** |
| æ‰‹å†™æ±‡ç¼– | 26 ns | **9.4x** |

### 1.6 å®æˆ˜ç»ƒä¹ 

#### ç»ƒä¹ 1: åˆ†æUSearchè·ç¦»è®¡ç®—

```bash
# 1. ç¼–è¯‘USearch C++æ ¸å¿ƒ
cd /home/dev/USearch
g++ -O3 -march=native -S -I include \
    python/lib.cpp -o python/lib.s

# 2. æŸ¥çœ‹ä½™å¼¦è·ç¦»çš„æ±‡ç¼–
grep -A 100 "cosine.*:" python/lib.s | less

# 3. æ ‡æ³¨çƒ­ç‚¹
perf record -g ./your_benchmark
perf annotate --stdio | grep -A 5 cosine

# ä»»åŠ¡:
# 1. è¯†åˆ«å¾ªç¯æ˜¯å¦å‘é‡åŒ–
# 2. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨FMA
# 3. ä¼˜åŒ–æ ‡é‡å°¾éƒ¨å¤„ç†
```

#### ç»ƒä¹ 2: ç¼–å†™SIMDå†…è”æ±‡ç¼–

```cpp
// ä»»åŠ¡: å®ç°AVX2ç‰ˆæœ¬çš„L2è·ç¦»
// æç¤º: ä½¿ç”¨vsubps, vfmadd231psæŒ‡ä»¤

float l2_avx2(const float* a, const float* b, size_t n) {
    __m256 sum = _mm256_setzero_ps();

    size_t i = 0;
    for (; i + 8 <= n; i += 8) {
        __m256 va = _mm256_loadu_ps(a + i);
        __m256 vb = _mm256_loadu_ps(b + i);

        // TODO: è®¡ç®—sum((a-b)^2)
        // 1. è®¡ç®—å·®å€¼: sub = a - b
        // 2. å¹³æ–¹å¹¶ç´¯åŠ : sum += sub * sub
    }

    // æ°´å¹³æ±‚å’Œ
    float result = hsum256_avx2(sum);

    // å¤„ç†å‰©ä½™å…ƒç´ 
    for (; i < n; i++) {
        float diff = a[i] - b[i];
        result += diff * diff;
    }

    return sqrtf(result);
}

// å‚è€ƒ: https://stackoverflow.com/questions/49941645/get-sum-of-values-stored-in-m256
```

#### ç»ƒä¹ 3: æ€§èƒ½å¯¹æ¯”å®éªŒ

```python
# benchmark_cosine.py
import numpy as np
import time

# ç”Ÿæˆé•¿åº¦ä¸º1000ä¸‡,768ç»´çš„å‘é‡
vectors = np.random.randn(10_000_000, 768).astype(np.float32)
query = vectors[0]

# æµ‹è¯•ä¸åŒå®ç°
implementations = [
    "scalar",
    "avx2",
    "avx512",
    "asm_optimized"
]

for impl in implementations:
    start = time.time()
    for i in range(1000):
        # è°ƒç”¨å¯¹åº”çš„C++å®ç°
        distance = cosine_distance(query, vectors[i], impl)
    elapsed = time.time() - start

    print(f"{impl}: {elapsed*1000:.2f}ms")

# ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾
import matplotlib.pyplot as plt
plt.bar(implementations, times)
plt.ylabel('Time (ms)')
plt.title('Cosine Distance Performance')
plt.savefig('cosine_benchmark.png')
```

---

## Advanced 2: CPUäº²å’Œæ€§ä¸NUMAä¼˜åŒ– ğŸ–¥ï¸

### 2.1 NUMAæ¶æ„æ·±åº¦è§£æ

**é—®é¢˜:** å¤šè·¯æœåŠ¡å™¨ä¸Š,æ€§èƒ½éšçº¿ç¨‹æ•°å¢åŠ åè€Œä¸‹é™

**åŸå› :** NUMA (Non-Uniform Memory Access)æ¶æ„

#### NUMAå†…å­˜è®¿é—®å»¶è¿Ÿ

```
å•è·¯æœåŠ¡å™¨ (UMA):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CPU (æ‰€æœ‰æ ¸å¿ƒ)            â”‚
â”‚   â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”        â”‚
â”‚   â”‚ 0 â”‚ 1 â”‚ 2 â”‚ 3 â”‚        â”‚
â”‚   â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜        â”‚
â”‚         â”‚                  â”‚
â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”             â”‚
â”‚    â”‚  Memory â”‚             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
æ‰€æœ‰æ ¸å¿ƒè®¿é—®å†…å­˜å»¶è¿Ÿç›¸åŒ: ~80ns

åŒè·¯æœåŠ¡å™¨ (NUMA):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Socket 0         â”‚  â”‚ Socket 1         â”‚
â”‚ â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”â”‚  â”‚â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”â”‚
â”‚ â”‚ 0 â”‚ 1 â”‚ 2 â”‚ 3 â”‚â”‚  â”‚â”‚ 4 â”‚ 5 â”‚ 6 â”‚ 7 â”‚â”‚
â”‚ â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜â”‚  â”‚â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜â”‚
â”‚      â”‚ QPI       â”‚  â”‚      â”‚ QPI       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”    â”‚  â”‚ â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”    â”‚
â”‚ â”‚ Local    â”‚    â”‚  â”‚ â”‚ Local    â”‚    â”‚
â”‚ â”‚ Memory 0 â”‚    â”‚  â”‚ â”‚ Memory 1 â”‚    â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚

è®¿é—®å»¶è¿Ÿ:
- æœ¬åœ°å†…å­˜: ~80ns
- è¿œç¨‹å†…å­˜: ~120-150ns (æ…¢50%!)
```

#### æ€§èƒ½é™·é˜±ç¤ºä¾‹

```cpp
// âŒ é”™è¯¯: çº¿ç¨‹åœ¨Socket 0, æ•°æ®åœ¨Socket 1
void bad_numa_pattern() {
    // çº¿ç¨‹0åœ¨Socket 0ä¸Šåˆ†é…å†…å­˜
    std::vector<float> data(100000000);

    // å¯åŠ¨çº¿ç¨‹1-7,åœ¨Socket 1ä¸Šè¿è¡Œ
    #pragma omp parallel for num_threads(8)
    for (int i = 0; i < 8; i++) {
        // çº¿ç¨‹4-7è®¿é—®è¿œç¨‹å†…å­˜!
        process_data(data);
    }
    // æ€§èƒ½æŸå¤±: 40-50%
}
```

### 2.2 NUMAæ„ŸçŸ¥åˆ†é…å™¨

**æºç ä½ç½®:** `index_plugins.hpp:1000+`

```cpp
#if defined(__linux__) && defined(__x86_64__)
#include <numa.h>

class numa_allocator_gt {
    int numa_node_;

public:
    using value_type = byte_t;

    explicit numa_allocator_gt(int preferred_node = -1)
        : numa_node_(preferred_node) {

        if (numa_available() < 0) {
            throw std::runtime_error("NUMA not available");
        }
    }

    byte_t* allocate(std::size_t n) const {
        void* ptr = nullptr;

        if (numa_node_ >= 0) {
            // åœ¨æŒ‡å®šNUMAèŠ‚ç‚¹åˆ†é…
            ptr = numa_alloc_onnode(n, numa_node_);
        } else {
            // æœ¬åœ°åˆ†é…
            ptr = numa_alloc_local(n);
        }

        if (!ptr) {
            throw std::bad_alloc();
        }

        return static_cast<byte_t*>(ptr);
    }

    void deallocate(byte_t* ptr, std::size_t n) const noexcept {
        numa_free(ptr, n);
    }

    // è·å–å½“å‰çº¿ç¨‹çš„NUMAèŠ‚ç‚¹
    static int get_current_node() {
        return numa_node_of_cpu(sched_getcpu());
    }
};

#endif
```

### 2.3 CPUäº²å’Œæ€§è®¾ç½®

```cpp
#include <pthread.h>
#include <sched.h>

class ThreadAffinity {
public:
    // ç»‘å®šçº¿ç¨‹åˆ°ç‰¹å®šCPUæ ¸å¿ƒ
    static void set_affinity(pthread_t thread, int core_id) {
        cpu_set_t cpuset;
        CPU_ZERO(&cpuset);
        CPU_SET(core_id, &cpuset);

        int rc = pthread_setaffinity_np(
            thread,
            sizeof(cpu_set_t),
            &cpuset
        );

        if (rc != 0) {
            perror("pthread_setaffinity_np");
        }
    }

    // ç»‘å®šåˆ°NUMAèŠ‚ç‚¹çš„æ‰€æœ‰æ ¸å¿ƒ
    static void set_numa_affinity(pthread_t thread, int numa_node) {
        cpu_set_t cpuset;
        CPU_ZERO(&cpuset);

        // è·å–NUMAèŠ‚ç‚¹çš„æ‰€æœ‰CPU
        for (int cpu = 0; cpu < sysconf(_SC_NPROCESSORS_ONLN); cpu++) {
            if (numa_node_of_cpu(cpu) == numa_node) {
                CPU_SET(cpu, &cpuset);
            }
        }

        pthread_setaffinity_np(thread, sizeof(cpu_set_t), &cpuset);
    }
};
```

**USearchä¸­çš„é›†æˆ:**

```cpp
// index.hpp: çº¿ç¨‹ä¸Šä¸‹æ–‡åˆå§‹åŒ–
context_t& context_for_thread(size_t thread_id) {
    if (thread_id >= thread_contexts_.size()) {
        thread_contexts_.resize(thread_id + 1);
    }

    context_t& ctx = thread_contexts_[thread_id];

    // é¦–æ¬¡ä½¿ç”¨æ—¶è®¾ç½®äº²å’Œæ€§
    if (!ctx.affinity_set) {
        int numa_node = numa_allocator_gt::get_current_node();
        ThreadAffinity::set_numa_affinity(
            pthread_self(),
            numa_node
        );

        // åœ¨æœ¬åœ°NUMAèŠ‚ç‚¹åˆ†é…ä¸Šä¸‹æ–‡ç¼“å†²åŒº
        numa_allocator_gt allocator(numa_node);
        ctx.visits.reserve(1024);
        ctx.top_candidates.reserve(128);

        ctx.affinity_set = true;
    }

    return ctx;
}
```

### 2.4 First-Touchç­–ç•¥

**å…³é”®åŸåˆ™:** å†…å­˜åº”è¯¥åœ¨é¦–æ¬¡è®¿é—®çš„çº¿ç¨‹æ‰€åœ¨NUMAèŠ‚ç‚¹åˆ†é…

```cpp
// âŒ é”™è¯¯: ä¸»çº¿ç¨‹åˆ†é…,å·¥ä½œçº¿ç¨‹è®¿é—®
void bad_first_touch() {
    const size_t N = 100000000;
    std::vector<float> data(N);  // ä¸»çº¿ç¨‹åˆ†é… (å¯èƒ½åœ¨Socket 0)

    #pragma omp parallel for
    for (size_t i = 0; i < N; i++) {
        data[i] = process(i);  // å·¥ä½œçº¿ç¨‹è®¿é—® (å¯èƒ½åœ¨Socket 1)
    }
}

// âœ… æ­£ç¡®: å·¥ä½œçº¿ç¨‹é¦–æ¬¡è®¿é—®
void good_first_touch() {
    const size_t N = 100000000;
    std::vector<float> data(N);

    // å¼ºåˆ¶æ¯ä¸ªçº¿ç¨‹åˆå§‹åŒ–è‡ªå·±çš„éƒ¨åˆ†
    #pragma omp parallel for schedule(static)
    for (size_t i = 0; i < N; i++) {
        data[i] = 0;  // é¦–æ¬¡touch,åœ¨æœ¬åœ°NUMAåˆ†é…
    }

    // åç»­å¤„ç†
    #pragma omp parallel for schedule(static)
    for (size_t i = 0; i < N; i++) {
        data[i] = process(i);
    }
}
```

**USearchä¸­çš„åº”ç”¨:**

```cpp
// index_dense.hpp: å‘é‡æ•°æ®åˆ†é…
template <typename scalar_at>
class index_dense_gt {
    // ç¡®ä¿åœ¨æ­£ç¡®çš„NUMAèŠ‚ç‚¹åˆ†é…
    void allocate_vectors() {
        size_t vectors_per_thread = (capacity_ + num_threads_ - 1) / num_threads_;

        #pragma omp parallel num_threads(num_threads_)
        {
            int thread_id = omp_get_thread_num();
            int numa_node = numa_node_of_cpu(sched_getcpu());

            // ä½¿ç”¨NUMAæ„ŸçŸ¥åˆ†é…å™¨
            numa_allocator_gt allocator(numa_node);

            size_t start = thread_id * vectors_per_thread;
            size_t end = std::min(start + vectors_per_thread, capacity_);

            for (size_t i = start; i < end; i++) {
                vectors_[i] = allocator.allocate(dimensions_ * sizeof(scalar_at));
            }
        }
    }
};
```

### 2.5 NUMAæ€§èƒ½æµ‹è¯•

**æµ‹è¯•ä»£ç :**

```cpp
#include <numa.h>
#include <omp.h>
#include <chrono>

void benchmark_numa() {
    const size_t N = 100000000;
    const size_t ITER = 100;

    printf("NUMA Benchmark\n");
    printf("NUMA nodes: %d\n", numa_num_configured_nodes());
    printf("CPUs: %d\n", numa_num_configured_cpus());
    printf("\n");

    // æµ‹è¯•1: æœ¬åœ°è®¿é—®
    auto test_local = [&]() {
        numa_allocator_gt allocator(numa_node_of_cpu(0));  // Socket 0
        float* data = (float*)allocator.allocate(N * sizeof(float));

        // ç»‘å®šåˆ°CPU 0 (Socket 0)
        ThreadAffinity::set_affinity(pthread_self(), 0);

        auto start = std::chrono::high_resolution_clock::now();

        for (size_t iter = 0; iter < ITER; iter++) {
            #pragma omp parallel for num_threads(4)
            for (int i = 0; i < 4; i++) {
                for (size_t j = i * N/4; j < (i+1) * N/4; j++) {
                    data[j] *= 1.01f;
                }
            }
        }

        auto end = std::chrono::high_resolution_clock::now();
        double elapsed = std::chrono::duration<double>(end - start).count();

        allocator.deallocate((byte_t*)data, N * sizeof(float));
        return elapsed;
    };

    // æµ‹è¯•2: è¿œç¨‹è®¿é—®
    auto test_remote = [&]() {
        numa_allocator_gt allocator(0);  // Socket 0åˆ†é…
        float* data = (float*)allocator.allocate(N * sizeof(float));

        // ç»‘å®šåˆ°CPU 4-7 (Socket 1)
        ThreadAffinity::set_affinity(pthread_self(), 4);

        auto start = std::chrono::high_resolution_clock::now();

        for (size_t iter = 0; iter < ITER; iter++) {
            #pragma omp parallel for num_threads(4)
            for (int i = 0; i < 4; i++) {
                for (size_t j = i * N/4; j < (i+1) * N/4; j++) {
                    data[j] *= 1.01f;
                }
            }
        }

        auto end = std::chrono::high_resolution_clock::now();
        double elapsed = std::chrono::duration<double>(end - start).count();

        allocator.deallocate((byte_t*)data, N * sizeof(float));
        return elapsed;
    };

    double local_time = test_local();
    double remote_time = test_remote();

    printf("Local access:  %.3f seconds\n", local_time);
    printf("Remote access: %.3f seconds\n", remote_time);
    printf("Slowdown:      %.2fx\n", remote_time / local_time);
}
```

**å…¸å‹ç»“æœ:**

```
NUMA Benchmark
NUMA nodes: 2
CPUs: 16

Local access:  2.34 seconds
Remote access: 3.51 seconds
Slowdown:      1.50x
```

### 2.6 å®æˆ˜ç»ƒä¹ 

#### ç»ƒä¹ 1: åˆ†æNUMAé…ç½®

```bash
# æŸ¥çœ‹NUMAæ‹“æ‰‘
numactl --hardware

# è¾“å‡ºç¤ºä¾‹:
# available: 2 nodes (0-1)
# node 0 cpus: 0 1 2 3 4 5 6 7
# node 0 size: 65536 MB
# node 1 cpus: 8 9 10 11 12 13 14 15
# node 1 size: 65536 MB

# æŸ¥çœ‹å½“å‰è¿›ç¨‹çš„NUMAç»Ÿè®¡
numastat -p $(pidof your_program)

# æŸ¥çœ‹å†…å­˜åˆ†å¸ƒ
numastat -m
```

#### ç»ƒä¹ 2: ä¼˜åŒ–USearchçš„NUMAæ€§èƒ½

```cpp
// ä»»åŠ¡: ä¿®æ”¹USearchä½¿å…¶NUMAæ„ŸçŸ¥
// 1. å®ç°numa_aware_index_gtç±»
// 2. åœ¨æ¯ä¸ªNUMAèŠ‚ç‚¹ç»´æŠ¤å­ç´¢å¼•
// 3. æœç´¢æ—¶ä¼˜å…ˆæŸ¥è¯¢æœ¬åœ°èŠ‚ç‚¹

template <typename index_at>
class numa_aware_index_gt {
    std::vector<index_at> sub_indexes_;  // æ¯ä¸ªNUMAèŠ‚ç‚¹ä¸€ä¸ª
    std::vector<int> numa_nodes_;

public:
    void add(key_t key, const float* vector) {
        int current_node = numa_node_of_cpu(sched_getcpu());
        sub_indexes_[current_node].add(key, vector);
    }

    std::vector<key_t> search(const float* vector, size_t k) {
        // 1. æœç´¢æœ¬åœ°èŠ‚ç‚¹ (å¿«é€Ÿ)
        int local_node = numa_node_of_cpu(sched_getcpu());
        auto local_results = sub_indexes_[local_node].search(vector, k);

        // 2. å¹¶è¡Œæœç´¢è¿œç¨‹èŠ‚ç‚¹
        std::vector<std::future<std::vector<key_t>>> futures;
        for (int node : numa_nodes_) {
            if (node != local_node) {
                futures.push_back(std::async(std::launch::async, [&]() {
                    return sub_indexes_[node].search(vector, k);
                }));
            }
        }

        // 3. åˆå¹¶ç»“æœ
        // TODO: å®ç°åˆå¹¶é€»è¾‘
    }
};
```

---

## Advanced 3: JITç¼–è¯‘ä¸åŠ¨æ€ä»£ç ç”Ÿæˆ âš¡

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦JIT?

**é—®é¢˜:** è‡ªå®šä¹‰è·ç¦»åº¦é‡æ€§èƒ½è¿œä½äºå†…ç½®åº¦é‡

```cpp
// è‡ªå®šä¹‰åº¦é‡
float my_distance(const float* a, const float* b, size_t n) {
    float sum = 0;
    for (size_t i = 0; i < n; i++) {
        float diff = a[i] - b[i];
        sum += diff * diff;
    }
    return sqrtf(sum);
}

// ä½¿ç”¨
index.add(metric = my_distance);  // æ€§èƒ½å·®!
```

**åŸå› :**
1. å‡½æ•°æŒ‡é’ˆè°ƒç”¨å¼€é”€
2. ç¼–è¯‘å™¨æ— æ³•è·¨å‡½æ•°è¾¹ç•Œä¼˜åŒ–
3. æ— æ³•å†…è”
4. æ— æ³•å‘é‡åŒ–

**è§£å†³:** LLVM JITç¼–è¯‘

### 3.2 LLVM JITåŸºç¡€

**å®‰è£…LLVM:**

```bash
# Ubuntu
sudo apt install libllvm-15-dev llvm-15-dev clang-15

# macOS
brew install llvm@15

# éªŒè¯
llvm-config --version
```

**Hello World JIT:**

```cpp
#include <llvm/ExecutionEngine/Orc/LLJIT.h>
#include <llvm/IR/Function.h>
#include <llvm/IR/IRBuilder.h>
#include <llvm/IR/Module.h>
#include <llvm/Support/TargetSelect.h>

using namespace llvm;
using namespace llvm::orc;

class JITCompiler {
    std::unique_ptr<LLJIT> jit_;
    llvm::orc::ThreadSafeContext context_;

public:
    JITCompiler()
        : context_(std::make_unique<llvm::LLVMContext>()) {

        // åˆå§‹åŒ–åŸç”Ÿç›®æ ‡
        InitializeNativeTarget();
        InitializeNativeTargetAsmPrinter();
        InitializeNativeTargetAsmParser();

        // åˆ›å»ºJIT
        auto jit = LLJITBuilder().create();
        if (!jit) {
            llvm::errs() << "Failed to create JIT: "
                        << toString(jit.takeError()) << "\n";
            exit(1);
        }
        jit_ = std::move(*jit);
    }

    // ç¼–è¯‘å¹¶è¿”å›å‡½æ•°æŒ‡é’ˆ
    template <typename Func_t>
    Func_t compile(std::unique_ptr<Module> module) {
        // æ·»åŠ æ¨¡å—åˆ°JIT
        auto err = jit_->addIRModule(
            ThreadSafeModule(std::move(module), context_)
        );

        if (err) {
            llvm::errs() << "Failed to add module: "
                        << toString(std::move(err)) << "\n";
            return nullptr;
        }

        // æŸ¥æ‰¾å‡½æ•°
        auto sym = jit_->lookup("main_func");
        if (!sym) {
            llvm::errs() << "Failed to lookup function: "
                        << toString(sym.takeError()) << "\n";
            return nullptr;
        }

        return reinterpret_cast<Func_t>(sym->getValue());
    }
};
```

### 3.3 IRç”Ÿæˆç¤ºä¾‹

**ç›®æ ‡:** ç”Ÿæˆä¼˜åŒ–çš„L2è·ç¦»å‡½æ•°

```cpp
std::unique_ptr<Module> generate_l2_distance_ir(
    LLVMContext& ctx,
    size_t dimensions
) {
    // åˆ›å»ºæ¨¡å—
    auto module = std::make_unique<Module>("l2_distance", ctx);
    module->setDataLayout(jit_->getDataLayout());

    // å‡½æ•°ç­¾å: float(float*, float*, size_t)
    Type* float_ty = Type::getFloatTy(ctx);
    Type* float_ptr_ty = Type::getFloatPtrTy(ctx);
    Type* size_ty = Type::getInt64Ty(ctx);

    FunctionType* ft = FunctionType::get(
        float_ty,
        {float_ptr_ty, float_ptr_ty, size_ty},
        false
    );

    Function* func = Function::Create(
        ft,
        GlobalValue::ExternalLinkage,
        "l2_distance_jit",
        module.get()
    );

    // å‚æ•°
    auto args = func->arg_begin();
    Value* a = &*args++;
    a->setName("a");
    Value* b = &*args++;
    b->setName("b");
    Value* n = &*args++;
    n->setName("n");

    // åŸºæœ¬å—
    BasicBlock* entry = BasicBlock::Create(ctx, "entry", func);
    BasicBlock* loop = BasicBlock::Create(ctx, "loop", func);
    BasicBlock* exit = BasicBlock::Create(ctx, "exit", func);

    IRBuilder<> builder(entry);

    // åˆå§‹åŒ–ç´¯åŠ å™¨
    Value* sum_ptr = builder.CreateAlloca(float_ty);
    builder.CreateStore(
        ConstantFP::get(float_ty, 0.0),
        sum_ptr
    );

    Value* i_ptr = builder.CreateAlloca(size_ty);
    builder.CreateStore(
        ConstantInt::get(size_ty, 0),
        i_ptr
    );

    // è·³è½¬åˆ°å¾ªç¯
    builder.CreateBr(loop);

    // å¾ªç¯ä½“
    builder.SetInsertPoint(loop);

    // i = load i
    Value* i = builder.CreateLoad(size_ty, i_ptr);
    Value* i_next = builder.CreateAdd(
        i,
        ConstantInt::get(size_ty, 1)
    );
    builder.CreateStore(i_next, i_ptr);

    // åŠ è½½a[i]å’Œb[i]
    Value* a_elem_ptr = builder.CreateGEP(
        float_ty,
        a,
        i
    );
    Value* b_elem_ptr = builder.CreateGEP(
        float_ty,
        b,
        i
    );

    Value* a_elem = builder.CreateLoad(float_ty, a_elem_ptr);
    Value* b_elem = builder.CreateLoad(float_ty, b_elem_ptr);

    // diff = a - b
    Value* diff = builder.CreateFSub(a_elem, b_elem);

    // sum += diff * diff
    Value* sum = builder.CreateLoad(float_ty, sum_ptr);
    Value* diff_sq = builder.CreateFMul(diff, diff);
    Value* new_sum = builder.CreateFAdd(sum, diff_sq);
    builder.CreateStore(new_sum, sum_ptr);

    // æ¡ä»¶è·³è½¬
    Value* cond = builder.CreateICmpULT(i_next, n);
    builder.CreateCondBr(cond, loop, exit);

    // é€€å‡ºå—
    builder.SetInsertPoint(exit);

    // return sqrt(sum)
    sum = builder.CreateLoad(float_ty, sum_ptr);

    // æ’å…¥sqrtè°ƒç”¨
    Function* sqrt_func = module->getFunction("sqrtf");
    if (!sqrt_func) {
        // å£°æ˜å¤–éƒ¨å‡½æ•°
        FunctionType* sqrt_ft = FunctionType::get(float_ty, {float_ty}, false);
        sqrt_func = Function::Create(
            sqrt_ft,
            GlobalValue::ExternalLinkage,
            "sqrtf",
            module.get()
        );
    }

    Value* result = builder.CreateCall(sqrt_func, {sum});
    builder.CreateRet(result);

    return module;
}
```

**ä½¿ç”¨JITç¼–è¯‘å™¨:**

```cpp
// ç¼–è¯‘
JITCompiler jit;
auto module = generate_l2_distance_ir(*context.getContext(), 768);

using L2Func = float(*)(const float*, const float*, size_t);
L2Func l2_jit = jit.compile<L2Func>(std::move(module));

// ä½¿ç”¨
const float* a = ...;
const float* b = ...;
float dist = l2_jit(a, b, 768);
```

### 3.4 è‡ªåŠ¨å‘é‡åŒ–

**å¯ç”¨LLVMè‡ªåŠ¨å‘é‡åŒ–:**

```cpp
// è®¾ç½®å¾ªç¯å±æ€§
loop->getIterator().setName("loop");
loop->addFnAttr(Attribute::AlwaysInline);  // å¼ºåˆ¶å†…è”

// æ·»åŠ å…ƒæ•°æ®æŒ‡ç¤ºå¯å‘é‡åŒ–
MDNode* md_node = MDNode::get(
    ctx,
    {ConstantInt::get(Type::getInt32Ty(ctx), 1)}
);
loop->setMetadata(
    "llvm.loop.vectorize.enable",
    md_node
);
```

**ç”Ÿæˆçš„æ±‡ç¼– (AVX2):**

```asm
# LLVMè‡ªåŠ¨ç”Ÿæˆ
l2_distance_jit:
    # å‘é‡åŒ–ä¸»å¾ªç¯
    vxorps  ymm0, ymm0, ymm0          # sum = 0
    xor     rax, rax
.LBB0_1:
    vmovups ymm1, YMMWORD PTR [rdi+rax*4]
    vmovups ymm2, YMMWORD PTR [rsi+rax*4]
    vsubps  ymm1, ymm1, ymm2          # diff = a - b
    vfmadd231ps ymm0, ymm1, ymm1      # sum += diff * diff
    add     rax, 8
    cmp     rax, rdx
    jb      .LBB0_1

    # æ°´å¹³å½’çº¦
    # ...
    vsqrtss xmm0, xmm0, xmm0
    ret
```

### 3.5 Numba JITé›†æˆ (Python)

USearchæ”¯æŒNumba JITç¼–è¯‘çš„åº¦é‡å‡½æ•°:

```python
import numba
import usearch
import numpy as np

# ä½¿ç”¨Numba JITç¼–è¯‘è‡ªå®šä¹‰åº¦é‡
@numba.njit(fastmath=True, cache=True)
def weighted_l2(a, b, weights):
    """å¸¦æƒé‡çš„L2è·ç¦»"""
    sum_sq = 0.0
    for i in range(len(a)):
        diff = (a[i] - b[i]) * weights[i]
        sum_sq += diff * diff
    return np.sqrt(sum_sq)

# ç¼–è¯‘
weights = np.ones(768, dtype=np.float32)
weights[:128] *= 2.0  # å‰128ç»´æƒé‡åŠ å€

# ä½¿ç”¨
index = usearch.Index(
    ndim=768,
    metric=weighted_l2,
    dtype='f32'
)

# Numbaå‡½æ•°ä¼šè¢«JITç¼–è¯‘ä¸ºæœºå™¨ç 
# æ€§èƒ½æ¥è¿‘å†…ç½®åº¦é‡!
```

### 3.6 å®æˆ˜ç»ƒä¹ 

#### ç»ƒä¹ 1: å®ç°é€šç”¨åº¦é‡JIT

```cpp
// ä»»åŠ¡: å®ç°ä¸€ä¸ªå¯ä»¥ç¼–è¯‘ä»»æ„åº¦é‡çš„JITç³»ç»Ÿ
// 1. è§£æè‡ªå®šä¹‰åº¦é‡è¯­è¨€ (DSL)
// 2. ç”ŸæˆLLVM IR
// 3. ç¼–è¯‘ä¸ºæœºå™¨ç 

// ç¤ºä¾‹DSL:
// "metric weighted_l2 {
//     dimension: 768;
//     weights: [1.0, 1.0, ..., 2.0, ...];
//     formula: sum((a[i] - b[i])^2 * weights[i]);
// }"

class MetricJIT {
    JITCompiler jit_;

public:
    using MetricFunc = float(*)(const float*, const float*, size_t);

    MetricFunc compile(const std::string& dsl) {
        // 1. è§£æDSL
        // 2. ç”ŸæˆLLVM IR
        // 3. ç¼–è¯‘å¹¶è¿”å›å‡½æ•°æŒ‡é’ˆ
    }
};
```

---

## ğŸ“š å­¦ä¹ èµ„æº

### æ±‡ç¼–ä¸ä¼˜åŒ–
- **Intel Intrinsics Guide:** https://www.intel.com/content/www/us/en/docs/intrinsics-guide/
- **Agner Fog's Optimization Manuals:** https://www.agner.org/optimize/
- **Compiler Explorer:** https://godbolt.org/

### NUMAç¼–ç¨‹
- **numa(7) Man Page:** `man numa`
- **Linux NUMA API:** https://linux.die.net/man/3/numa
- **"What Every Programmer Should Know About Memory"** - Ulrich Drepper

### LLVM JIT
- **LLVM Documentation:** https://llvm.org/docs/
- **Kaleidoscope Tutorial:** https://llvm.org/docs/tutorial/
- **USearch Numba Integration:** `python/lib.cpp`

---

## ğŸ“ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬è¯¾ç¨‹å,ä½ åº”è¯¥èƒ½å¤Ÿ:
1. é˜…è¯»å’Œåˆ†ææ±‡ç¼–ä»£ç 
2. è¯†åˆ«å’Œä¿®å¤NUMAæ€§èƒ½é—®é¢˜
3. ä½¿ç”¨LLVMå®ç°JITç¼–è¯‘
4. ä¼˜åŒ–CPUç¼“å­˜äº²å’Œæ€§

**æ¨èé¡¹ç›®:**
1. ä¸ºUSearchå®ç°GPUåŠ é€Ÿ (CUDA)
2. å®ç°åˆ†å¸ƒå¼HNSW
3. æ„å»ºå®æ—¶ç´¢å¼•æ›´æ–°ç³»ç»Ÿ

---

**æŒç»­æ›´æ–°ä¸­...** ğŸš€
