# USearch æ€§èƒ½ä¼˜åŒ–å®æˆ˜æŒ‡å—
## ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´ä¼˜åŒ–è·¯å¾„

---

## ğŸ¯ ç›®æ ‡

- å°†ç†è®ºçŸ¥è¯†è½¬åŒ–ä¸ºå®é™…ä¼˜åŒ–
- æŒæ¡æ€§èƒ½åˆ†æå·¥å…·é“¾
- å­¦ä¹ ç³»ç»ŸåŒ–çš„ä¼˜åŒ–æ–¹æ³•
- è·å¾—å¯é‡åŒ–çš„æ€§èƒ½æå‡

---

## 1. æ€§èƒ½åˆ†æå·¥å…·é“¾

### 1.1 å·¥å…·å…¨æ™¯å›¾

```
åº”ç”¨æ€§èƒ½åˆ†æ
    â”œâ”€â”€ CPU çº§åˆ«
    â”‚   â”œâ”€â”€ perf (Linux)
    â”‚   â”œâ”€â”€ VTune (Intel)
    â”‚   â””â”€â”€ sample (Apple)
    â”‚
    â”œâ”€â”€ å†…å­˜çº§åˆ¥
    â”‚   â”œâ”€â”€ Valgrind (Memcheck)
    â”‚   â”œâ”€â”€ Valgrind (Massif)
    â”‚   â””â”€â”€ heaptrack
    â”‚
    â”œâ”€â”€ ç¼“å­˜çº§åˆ¥
    â”‚   â”œâ”€â”€ perf (cache events)
    â”‚   â”œâ”€â”€ VTune (Cache Analysis)
    â”‚   â””â”€â”€ objdump (åæ±‡ç¼–)
    â”‚
    â””â”€â”€ åº”ç”¨çº§åˆ¥
        â”œâ”€â”€ Flame Graph
        â”œâ”€â”€ Chrome Tracing
        â””â”€â”€ Custom profiling
```

### 1.2 å¿«é€Ÿè¯Šæ–­æµç¨‹

```
ç¬¬ä¸€æ­¥ï¼šç²—ç•¥å®šä½
  $ time ./your_program

  real    0m5.234s
  user    4.890s
  sys     0.344s

  åˆ¤æ–­ï¼šCPUå¯†é›†å‹

ç¬¬äºŒæ­¥ï¼šè¯¦ç»†åˆ†æ
  $ perf record -g ./your_program
  $ perf report

  æŸ¥çœ‹çƒ­ç‚¹å‡½æ•°

ç¬¬ä¸‰æ­¥ï¼šæ·±å…¥åˆ†æ
  $ perf stat -e cache-references,cache-misses ./your_program

  åˆ†æç¼“å­˜è¡Œä¸º

ç¬¬å››æ­¥ï¼šä¼˜åŒ–å®æ–½
  æ ¹æ®åˆ†æç»“æœå®æ–½ä¼˜åŒ–

ç¬¬äº”æ­¥ï¼šéªŒè¯æ•ˆæœ
  å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½æŒ‡æ ‡
```

---

## 2. çƒ­ç‚¹å‡½æ•°åˆ†æ

### 2.1 è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ

**åœºæ™¯1ï¼šè·ç¦»è®¡ç®—æ˜¯ç“¶é¢ˆ**

```bash
# è¿è¡Œæ€§èƒ½åˆ†æ
$ perf record -F 99 -g ./bench_cpp
$ perf report --stdio

# è¾“å‡ºï¼š
34.52%  cosine_f32_scalar
    cosine_f32_scalar()
    ::distances
    search_to_insert_()

22.18%  l2_sq_f32_scalar
    l2_sq_f32_scalar()
    ::distances
    search_to_insert_()

15.37%  add_edge
    add_edge()
    ::insert_node
```

**ç»“è®º**ï¼šè·ç¦»è®¡ç®—å  56.7% çš„æ—¶é—´ï¼Œå¿…é¡»ä¼˜åŒ–ï¼

### 2.2 ä¼˜åŒ–ç­–ç•¥

**ç­–ç•¥1ï¼šå¯ç”¨ SIMD**

```cpp
// ä¼˜åŒ–å‰ï¼šæ ‡é‡ç‰ˆæœ¬
float cosine_f32_scalar(float const* a, float const* b, std::size_t n) {
    float ab = 0, a2 = 0, b2 = 0;
    for (std::size_t i = 0; i < n; ++i) {
        ab += a[i] * b[i];
        a2 += a[i] * a[i];
        b2 += b[i] * b[i];
    }
    return ab / std::sqrt(a2 * b2);
}

// æ€§èƒ½ï¼š180 ns per call
```

```cpp
// ä¼˜åŒ–åï¼šSIMD ç‰ˆæœ¬ï¼ˆä½¿ç”¨ SimSIMDï¼‰
float cosine_f32_simd(float const* a, float const* b, std::size_t n) {
    simsimd_capability_t caps = simsimd_cap_hardware;
    simsimd_metric_punned_t metric = simsimd_metric_cos_f32;

    // è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æŒ‡ä»¤é›†ï¼ˆAVX2/AVX-512/NEONï¼‰
    simsimd_cos_f32(a, b, n, &caps, metric);
}

// æ€§èƒ½ï¼š25 ns per call
// åŠ é€Ÿæ¯”ï¼š7.2x
```

**å®æ–½æ­¥éª¤**ï¼š

1. **æ£€æµ‹ç¡¬ä»¶èƒ½åŠ›**ï¼š
```cpp
simsimd_capability_t caps = simsimd_cap_hardware;
std::cout << "Hardware capabilities: " << caps << "\n";
```

2. **é€‰æ‹©æœ€ä¼˜å†…æ ¸**ï¼š
```cpp
simsimd_metric_punned_t metric = NULL;
simsimd_find_kernel_punned(
    simsimd_metric_cos_k,
    simsimd_datatype_f32_k,
    caps,
    allowed,
    &metric,
    &used_kind
);
```

3. **è°ƒç”¨ä¼˜åŒ–çš„è·ç¦»å‡½æ•°**ï¼š
```cpp
float distance = metric(a, b, dimensions);
```

---

## 3. å†…å­˜ä¼˜åŒ–å®æˆ˜

### 3.1 é—®é¢˜è¯Šæ–­

**ç—‡çŠ¶**ï¼šæ€§èƒ½ä½äºé¢„æœŸï¼ŒCPU ä½¿ç”¨ç‡ä¸é«˜

```bash
# åˆ†æç¼“å­˜è¡Œä¸º
$ perf stat -e cache-references,cache-misses,cycles,instructions ./bench_cpp

# è¾“å‡ºï¼š
cache references:      2,345,678
cache misses:           923,456  (39.36% of all cache references)
cycles:                4,567,890
instructions:          1,234,567
IPC:                   0.27  (å¾ˆä½ï¼)
```

**åˆ†æ**ï¼š
- ç¼“å­˜æœªå‘½ä¸­ç‡ 39%ï¼ˆåº”è¯¥ < 10%ï¼‰
- IPC åªæœ‰ 0.27ï¼ˆåº”è¯¥ > 1.0ï¼‰
- è¯´æ˜å†…å­˜è®¿é—®æ˜¯ç“¶é¢ˆ

### 3.2 ä¼˜åŒ–æ–¹æ¡ˆ

**æ–¹æ¡ˆ1ï¼šæ”¹å–„æ•°æ®å±€éƒ¨æ€§**

```cpp
// âŒ ä¸å¥½ï¼šAoSï¼ˆArray of Structuresï¼‰
struct Node {
    vector_key_t key;
    level_t level;
    float vector[128];
    uint32_t neighbors[16];
    // æ‰€æœ‰æ•°æ®æ··åœ¨ä¸€èµ·
};
std::vector<Node> nodes;

// è®¿é—®æ¨¡å¼ï¼ˆä¸è¿ç»­ï¼‰ï¼š
for (auto& node : nodes) {
    process(node.key);  // è·³è¿‡ vector å’Œ neighbors
}
```

```cpp
// âœ… å¥½ï¼šSoAï¼ˆStructure of Arraysï¼‰
buffer_gt<vector_key_t> keys;     // è¿ç»­å­˜å‚¨
buffer_gt<level_t> levels;        // è¿ç»­å­˜å‚¨
buffer_gt<float> vectors;         // è¿ç»­å­˜å‚¨ï¼ˆåˆ†å—ï¼‰
buffer_gt<uint32_t> neighbors;   // è¿ç»­å­˜å‚¨ï¼ˆåˆ†å—ï¼‰

// è®¿é—®æ¨¡å¼ï¼ˆè¿ç»­ï¼‰ï¼š
for (std::size_t i = 0; i < count; ++i) {
    process(keys[i]);  // ç´§å¯†æ’åˆ—ï¼Œç¼“å­˜å‹å¥½
}
```

**æ€§èƒ½æå‡**ï¼š2-3x

**æ–¹æ¡ˆ2ï¼šç¼“å­˜è¡Œå¯¹é½**

```cpp
// âŒ æœªå¯¹é½
struct Node {
    vector_key_t key;      // 8 bytes
    level_t level;         // 2 bytes
    // padding: 6 bytes
    float vector[128];     // 512 bytes
};  // æ€»è®¡ï¼š528 bytesï¼ˆä¸æ˜¯64çš„å€æ•°ï¼‰

// âœ… å¯¹é½åˆ° 64 å­—èŠ‚
struct alignas(64) AlignedNode {
    vector_key_t key;
    level_t level;
    // padding: 54 bytes (æ˜¾å¼æˆ–éšå¼)
    float vector[128];
};  // æ€»è®¡ï¼š576 bytesï¼ˆ64çš„å€æ•°ï¼‰

// æˆ–è€…ä½¿ç”¨ç¼–è¯‘å™¨å±æ€§
struct Node {
    vector_key_t key;
    level_t level;
    float vector[128];
} __attribute__((aligned(64)));
```

**æ€§èƒ½æå‡**ï¼š10-20%

**æ–¹æ¡ˆ3ï¼šé¢„å–**

```cpp
// è½¯ä»¶é¢„å–
for (std::size_t i = 0; i < n; ++i) {
    // é¢„å–ä¸‹ä¸€ä¸ªå…ƒç´ 
    if (i + 1 < n) {
        __builtin_prefetch(&nodes[i + 1], 0, 3);
    }

    // å¤„ç†å½“å‰å…ƒç´ 
    process(nodes[i]);
}
```

**é«˜çº§é¢„å–**ï¼š

```cpp
// è‡ªé€‚åº”é¢„å–è·ç¦»
class AdaptivePrefetcher {
    std::size_t distance_ = 1;

public:
    void update(bool cache_miss) {
        if (cache_miss && distance_ < 16) {
            distance_ *= 2;  // å¢å¤§é¢„å–è·ç¦»
        } else if (!cache_miss && distance_ > 1) {
            distance_ /= 2;  // å‡å°é¢„å–è·ç¦»
        }
    }

    std::size_t get_distance() const { return distance_; }
};
```

---

## 4. å¹¶å‘ä¼˜åŒ–å®æˆ˜

### 4.1 é—®é¢˜ï¼šå¤šçº¿ç¨‹æ‰©å±•æ€§å·®

**ç—‡çŠ¶**ï¼š
```
1 çº¿ç¨‹: 10000 QPS
2 çº¿ç¨‹: 15000 QPS (æœŸæœ› 20000)
4 çº¿ç¨‹: 18000 QPS (æœŸæœ› 40000)
8 çº¿ç¨‹: 19000 QPS (æœŸæœ› 80000)
```

**è¯Šæ–­**ï¼š

```cpp
// å¯èƒ½çš„åŸå› ï¼š
// 1. å…¨å±€é”ç«äº‰
// 2. false sharing
// 3. å†…å­˜å¸¦å®½é¥±å’Œ
```

### 4.2 ä¼˜åŒ–æ–¹æ¡ˆ

**æ–¹æ¡ˆ1ï¼šå‡å°‘é”ç²’åº¦**

```cpp
// âŒ å…¨å±€é”
class GlobalLockIndex {
    std::mutex mutex_;

    void add(int key, float* vector) {
        std::lock_guard<std::mutex> lock(mutex_);
        // æ‰€æœ‰æ“ä½œéƒ½é”ä½
    }
};

// æ‰©å±•æ€§ï¼š1.5x (2çº¿ç¨‹)
```

```cpp
// âœ… èŠ‚ç‚¹çº§é”
class NodeLockIndex {
    buffer_gt<mutex_gt> mutexes_;

    void add(int key, float* vector) {
        compressed_slot_t slot = get_slot(key);
        node_lock_t lock(mutexes_, slot);  // åªé”å½“å‰èŠ‚ç‚¹

        // å…¶ä»–çº¿ç¨‹å¯ä»¥æ“ä½œä¸åŒèŠ‚ç‚¹
    }
};

// æ‰©å±•æ€§ï¼š6x (8çº¿ç¨‹)
```

**æ–¹æ¡ˆ2ï¼šæ— é”æ•°æ®ç»“æ„**

```cpp
// åŸå­è®¡æ•°å™¨
class LockFreeIndex {
    std::atomic<std::size_t> size_{0};

    void add(int key, float* vector) {
        // åŸå­æ“ä½œï¼ˆCASï¼‰
        std::size_t slot = size_.fetch_add(1, std::memory_order_relaxed);

        // æ— é”å†™å…¥
        nodes_[slot] = create_node(key, vector);
    }
};

// æ‰©å±•æ€§ï¼š7x (8çº¿ç¨‹)
```

**æ–¹æ¡ˆ3ï¼šæ‰¹é‡å¤„ç†**

```cpp
// æ‰¹é‡æ·»åŠ ï¼ˆå‡å°‘é”æ“ä½œï¼‰
void add_many(std::vector<int> keys, std::vector<float*> vectors) {
    const std::size_t batch_size = 1000;

    for (std::size_t i = 0; i < keys.size(); i += batch_size) {
        std::size_t end = std::min(i + batch_size, keys.size());

        // æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ª batch
        #pragma omp parallel for
        for (std::size_t j = i; j < end; ++j) {
            add(keys[j], vectors[j]);
        }
    }
}

// æ‰©å±•æ€§ï¼šæ¥è¿‘çº¿æ€§
```

---

## 5. ç¼–è¯‘ä¼˜åŒ–

### 5.1 ç¼–è¯‘å™¨æ ‡å¿—

```bash
# åŸºç¡€ä¼˜åŒ–ï¼ˆO2ï¼‰
-O2 -DNDEBUG

# æœ€é«˜çº§åˆ«ä¼˜åŒ–ï¼ˆO3ï¼‰
-O3 -DNDEBUG

# ç›®æ ‡æ¶æ„ä¼˜åŒ–
-march=native          # å½“å‰ CPU
-march=haswell         # Intel Haswell
-march=znver3          # AMD Zen3

# é“¾æ¥æ—¶ä¼˜åŒ–ï¼ˆLTOï¼‰
-flto=auto

# é…ç½®æ–‡ä»¶å¯¼å‘ä¼˜åŒ–ï¼ˆPGOï¼‰
-fprofile-generate     # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆ profile
-fprofile-use          # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ profile
```

### 5.2 å®é™…æ¡ˆä¾‹

**æµ‹è¯•ä»£ç **ï¼š

```cpp
#include <usearch/index.hpp>
#include <benchmark/benchmark.h>

static void BM_DistanceCalculation(benchmark::State& state) {
    std::vector<float> a(128, 1.0f);
    std::vector<float> b(128, 2.0f);

    for (auto _ : state) {
        float sum = 0;
        for (std::size_t i = 0; i < 128; ++i) {
            sum += a[i] * b[i];
        }
        benchmark::DoNotOptimize(sum);
    }
}

BENCHMARK(BM_DistanceCalculation);
BENCHMARK_MAIN();
```

**ç¼–è¯‘æµ‹è¯•**ï¼š

```bash
# æ— ä¼˜åŒ–
g++ -O0 test.cpp -lbenchmark -o test_O0
./test_O0
# ç»“æœ: ~500 ns per iteration

# O2 ä¼˜åŒ–
g++ -O2 test.cpp -lbenchmark -o test_O2
./test_O2
# ç»“æœ: ~50 ns per iteration (10x åŠ é€Ÿ)

# O3 + march=native
g++ -O3 -march=native test.cpp -lbenchmark -o test_O3
./test_O3
# ç»“æœ: ~30 ns per iteration (16x åŠ é€Ÿ)

# O3 + march=native + LTO
g++ -O3 -march=native -flto test.cpp -lbenchmark -o test_LTO
./test_LTO
# ç»“æœ: ~25 ns per iteration (20x åŠ é€Ÿ)
```

---

## 6. é‡åŒ–å®æˆ˜

### 6.1 ç²¾åº¦-æ€§èƒ½æƒè¡¡

**å®éªŒï¼šSIFT-1M æ•°æ®é›†ï¼ˆ100ä¸‡ 128ç»´å‘é‡ï¼‰**

| æ•°æ®ç±»å‹ | å†…å­˜ | Recall@10 | æœç´¢å»¶è¿Ÿ |
|---------|------|-----------|----------|
| f64 | 800 MB | 0.96 | 0.10 ms |
| f32 | 400 MB | 0.96 | 0.10 ms |
| f16 | 200 MB | 0.95 | 0.08 ms |
| bf16 | 200 MB | 0.95 | 0.08 ms |
| i8 | 100 MB | 0.91 | 0.07 ms |
| b1x8 | 12.5 MB | 0.75 | 0.05 ms |

**ä¼˜åŒ–å»ºè®®**ï¼š

1. **å†…å­˜å—é™**ï¼šä½¿ç”¨ f16 æˆ– i8
   - å†…å­˜èŠ‚çœï¼š50-75%
   - ç²¾åº¦æŸå¤±ï¼š< 5%
   - é€Ÿåº¦æå‡ï¼š10-30%

2. **å»¶è¿Ÿæ•æ„Ÿ**ï¼šä½¿ç”¨ f16 + SIMD
   - å†…å­˜èŠ‚çœï¼š50%
   - ç²¾åº¦æŸå¤±ï¼š< 1%
   - é€Ÿåº¦æå‡ï¼š15-25%

3. **æåº¦å—é™**ï¼šä½¿ç”¨ i8 æˆ– PQ
   - å†…å­˜èŠ‚çœï¼š75-95%
   - ç²¾åº¦æŸå¤±ï¼š5-25%
   - éœ€è¦é‡æ’åº

### 6.2 é‡åŒ–å®æ–½

**æ­¥éª¤1ï¼šè¯„ä¼°å½±å“**

```python
import numpy as np
import usearch

# åˆ›å»º f32 ç´¢å¼•
index_f32 = usearch.Index(ndim=128, metric='cos', dtype='f32')
# ... æ·»åŠ æ•°æ® ...

# æµ‹è¯• f32 æ€§èƒ½
latency_f32 = measure_latency(index_f32)
recall_f32 = measure_recall(index_f32)

print(f"f32: {latency_f32:.3f} ms, Recall: {recall_f32:.3f}")
```

**æ­¥éª¤2ï¼šå°è¯• f16**

```python
# åˆ›å»º f16 ç´¢å¼•
index_f16 = usearch.Index(ndim=128, metric='cos', dtype='f16')

# æ·»åŠ ç›¸åŒçš„æ•°æ®ï¼ˆè‡ªåŠ¨é‡åŒ–ï¼‰
index_f16.add(keys, vectors_f32)  # è‡ªåŠ¨è½¬æ¢ä¸º f16

# æµ‹è¯• f16 æ€§èƒ½
latency_f16 = measure_latency(index_f16)
recall_f16 = measure_recall(index_f16)

memory_f32 = get_memory_usage(index_f32)
memory_f16 = get_memory_usage(index_f16)

print(f"f16: {latency_f16:.3f} ms, Recall: {recall_f16:.3f}")
print(f"å†…å­˜èŠ‚çœ: {(1 - memory_f16/memory_f32)*100:.1f}%")
```

**æ­¥éª¤3ï¼šå†³ç­–**

```python
if recall_f16 >= 0.95:
    print("âœ“ ä½¿ç”¨ f16")
else:
    print("âœ— ä¿æŒ f32ï¼ˆç²¾åº¦è¦æ±‚é«˜ï¼‰")
```

---

## 7. ç«¯åˆ°ç«¯ä¼˜åŒ–æ¡ˆä¾‹

### 7.1 æ¡ˆä¾‹ï¼šä¼˜åŒ–å›¾åƒæœç´¢ç³»ç»Ÿ

**åˆå§‹æ€§èƒ½**ï¼š
```
ç´¢å¼•æ„å»ºï¼š10ä¸‡å¼ å›¾ç‰‡ï¼Œè€—æ—¶ 5 åˆ†é’Ÿ
æœç´¢å»¶è¿Ÿï¼šå¹³å‡ 200ms
å†…å­˜ä½¿ç”¨ï¼š2 GB
```

**ä¼˜åŒ–è¿‡ç¨‹**ï¼š

**ä¼˜åŒ–1ï¼šå¯ç”¨ SIMD**
```python
# æ£€æŸ¥ç¡¬ä»¶åŠ é€Ÿ
print(index.hardware_acceleration)

# ç»“æœï¼šä» none â†’ avx2
# æœç´¢å»¶è¿Ÿï¼š200ms â†’ 150ms (1.33x)
```

**ä¼˜åŒ–2ï¼šè°ƒæ•´å‚æ•°**
```python
# é™ä½ Mï¼ˆè¿æ¥æ•°ï¼‰
index = usearch.Index(connectivity=8)  # ä» 16 é™åˆ° 8

# æœç´¢å»¶è¿Ÿï¼š150ms â†’ 100ms (1.5x)
# å†…å­˜ä½¿ç”¨ï¼š2GB â†’ 1.2GB
```

**ä¼˜åŒ–3ï¼šä½¿ç”¨ f16 é‡åŒ–**
```python
index = usearch.Index(dtype='f16')

# æœç´¢å»¶è¿Ÿï¼š100ms â†’ 80ms
# å†…å­˜ä½¿ç”¨ï¼š1.2GB â†’ 600MB
```

**ä¼˜åŒ–4ï¼šæ‰¹é‡æœç´¢**
```python
from concurrent.futures import ThreadPoolExecutor

def batch_search(index, queries, k=10, n_workers=4):
    with ThreadPoolExecutor(max_workers=n_workers) as executor:
        results = executor.map(lambda q: index.search(q, k), queries)
    return list(results)

# QPSï¼š10 â†’ 35 (3.5x)
```

**æœ€ç»ˆæ€§èƒ½**ï¼š
```
ç´¢å¼•æ„å»ºï¼š5 åˆ†é’Ÿ â†’ 2 åˆ†é’Ÿ
æœç´¢å»¶è¿Ÿï¼š200ms â†’ 80ms (å•çº¿ç¨‹)
QPSï¼š10 â†’ 35 (å¤šçº¿ç¨‹)
å†…å­˜ä½¿ç”¨ï¼š2GB â†’ 600MB

æ€»ä½“åŠ é€Ÿï¼š2.5x (å•çº¿ç¨‹), 8.75x (å¹¶å‘)
```

### 7.2 æ¡ˆä¾‹ï¼šä¼˜åŒ–æ¨èç³»ç»Ÿ

**åœºæ™¯**ï¼š100ä¸‡ç”¨æˆ·ï¼Œ10ä¸‡ç‰©å“

**åˆå§‹æ€§èƒ½**ï¼š
```
ç´¢å¼•å¤§å°ï¼š8 GB
æ›´æ–°å»¶è¿Ÿï¼š100ms
æœç´¢å»¶è¿Ÿï¼š50ms
```

**ä¼˜åŒ–ç­–ç•¥**ï¼š

**ä¼˜åŒ–1ï¼šä½¿ç”¨ i8 é‡åŒ–**
```python
index = usearch.Index(ndim=64, metric='ip', dtype='i8')

# ç´¢å¼•å¤§å°ï¼š8GB â†’ 2GB (4x å‹ç¼©)
# æœç´¢ç²¾åº¦ï¼šRecall 0.94 â†’ 0.91
```

**ä¼˜åŒ–2ï¼šåˆ†ç‰‡ç´¢å¼•**
```python
# æŒ‰ç±»åˆ«åˆ†ç‰‡
indexes = {
    'electronics': usearch.Index(...),
    'clothing': usearch.Index(...),
    'books': usearch.Index(...)
}

# åªæœç´¢ç›¸å…³ç±»åˆ«
def search(category, user_vector, k=10):
    return indexes[category].search(user_vector, k)

# æœç´¢å»¶è¿Ÿï¼š50ms â†’ 20ms (åªæœç´¢ä¸€ä¸ªåˆ†ç‰‡)
```

**ä¼˜åŒ–3ï¼šç¼“å­˜çƒ­é—¨æŸ¥è¯¢**
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def search_cached(user_id, k=10):
    return search(user_id, k)

# ç¼“å­˜å‘½ä¸­ç‡ï¼š40%
# å¹³å‡å»¶è¿Ÿï¼š20ms â†’ 12ms
```

**æœ€ç»ˆæ€§èƒ½**ï¼š
```
ç´¢å¼•å¤§å°ï¼š8GB â†’ 2GB (4x)
æœç´¢å»¶è¿Ÿï¼š50ms â†’ 12ms (4.17x)
æ›´æ–°å»¶è¿Ÿï¼š100ms â†’ 50ms
```

---

## 8. æ€§èƒ½æµ‹è¯•æ¡†æ¶

### 8.1 åŸºå‡†æµ‹è¯•æ¨¡æ¿

```cpp
#include <usearch/index.hpp>
#include <benchmark/benchmark.h>
#include <vector>

// åŸºå‡†æµ‹è¯•ï¼šæœç´¢æ€§èƒ½
static void BM_Search(benchmark::State& state) {
    using namespace unum::usearch;

    // å‡†å¤‡æ•°æ®
    index_dense_gt<float, std::uint32_t> index;
    index.init(128, metric_kind_t::cos_k, scalar_kind_t::f32_k);

    const std::size_t n_vectors = 100000;
    std::vector<float> vectors(n_vectors * 128);
    std::vector<std::uint32_t> keys(n_vectors);
    index.add(keys.data(), vectors.data(), n_vectors);

    // æŸ¥è¯¢å‘é‡
    std::vector<float> query(128, 1.0f);

    // é¢„çƒ­
    for (int i = 0; i < 100; ++i) {
        index.search(query.data(), 10);
    }

    // åŸºå‡†æµ‹è¯•
    for (auto _ : state) {
        auto results = index.search(query.data(), 10);
        benchmark::DoNotOptimize(results);
    }
}

// åŸºå‡†æµ‹è¯•ï¼šæ‰¹é‡æ’å…¥
static void BM_BatchInsert(benchmark::State& state) {
    using namespace unum::usearch;

    const std::size_t batch_size = 1000;
    std::vector<float> vectors(batch_size * 128);
    std::vector<std::uint32_t> keys(batch_size);

    for (auto _ : state) {
        state.PauseTiming();
        // åªæµ‹è¯•æ’å…¥ï¼Œä¸æµ‹è¯•ç´¢å¼•æ„å»º
        state.ResumeTiming();

        index_dense_gt<float, std::uint32_t> index;
        index.init(128, metric_kind_t::cos_k);
        index.add(keys.data(), vectors.data(), batch_size);
    }
}

BENCHMARK(BM_Search);
BENCHMARK(BM_BatchInsert);
BENCHMARK_MAIN();
```

**ç¼–è¯‘å’Œè¿è¡Œ**ï¼š

```bash
# å®‰è£… Google Benchmark
git clone https://github.com/google/benchmark.git
cd benchmark && mkdir build && cd build
cmake .. && make && make install

# ç¼–è¯‘åŸºå‡†æµ‹è¯•
g++ -std=c++17 -O3 -march=native \
    -I../../../include \
    benchmark_search.cpp \
    -lbenchmark \
    -lpthread \
    -o benchmark_search

# è¿è¡Œ
./benchmark_search --benchmark_repetitions=10
```

### 8.2 æ€§èƒ½å¯¹æ¯”å·¥å…·

```python
import usearch
import numpy as np
import time
import matplotlib.pyplot as plt

def compare_configurations():
    """å¯¹æ¯”ä¸åŒé…ç½®çš„æ€§èƒ½"""

    dimensions = 128
    n_vectors = 10000
    n_queries = 1000

    vectors = np.random.rand(n_vectors, dimensions).astype(np.float32)
    queries = np.random.rand(n_queries, dimensions).astype(np.float32)

    configs = [
        {'M': 8, 'ef': 16, 'dtype': 'f32'},
        {'M': 16, 'ef': 32, 'dtype': 'f32'},
        {'M': 16, 'ef': 64, 'dtype': 'f32'},
        {'M': 16, 'ef': 64, 'dtype': 'f16'},
    ]

    results = []

    for config in configs:
        # åˆ›å»ºç´¢å¼•
        index = usearch.Index(
            ndim=dimensions,
            metric='cos',
            connectivity=config['M'],
            expansion=config['ef'],
            dtype=config['dtype']
        )

        # æ·»åŠ å‘é‡
        keys = np.arange(n_vectors, dtype=np.uint64)
        start = time.time()
        index.add(keys, vectors)
        build_time = time.time() - start

        # æµ‹è¯•æœç´¢
        latencies = []
        for query in queries:
            start = time.perf_counter()
            index.search(query, 10)
            latencies.append(time.perf_counter() - start)

        avg_latency = np.mean(latencies) * 1000  # ms

        results.append({
            'config': config,
            'build_time': build_time,
            'avg_latency_ms': avg_latency,
            'qps': n_queries / sum(latencies)
        })

    # å¯è§†åŒ–
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    configs_str = [str(r['config']) for r in results]
    build_times = [r['build_time'] for r in results]
    latencies = [r['avg_latency_ms'] for r in results]

    ax1.bar(configs_str, build_times)
    ax1.set_ylabel('Build Time (s)')
    ax1.set_title('Index Build Time')
    ax1.tick_params(axis='x', rotation=45)

    ax2.bar(configs_str, latencies)
    ax2.set_ylabel('Avg Latency (ms)')
    ax2.set_title('Search Latency')
    ax2.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.savefig('config_comparison.png', dpi=300)
    plt.show()

    return results

if __name__ == '__main__':
    results = compare_configurations()
    for r in results:
        print(f"{r['config']}: Build={r['build_time']:.2f}s, "
              f"Latency={r['avg_latency_ms']:.3f}ms, QPS={r['qps']:.0f}")
```

---

## 9. æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

### ç¼–è¯‘æ—¶ä¼˜åŒ–
- [ ] ä½¿ç”¨ Release æ¨¡å¼ (-O3)
- [ ] æŒ‡å®šç›®æ ‡æ¶æ„ (-march=native)
- [ ] å¯ç”¨ LTO (-flto=auto)
- [ ] å¯ç”¨ PGO (fprofile-generate/use)
- [ ] å¯ç”¨ SIMD (-DUSEARCH_USE_SIMSIMD=1)

### ç®—æ³•ä¼˜åŒ–
- [ ] é€‰æ‹©åˆé€‚çš„è·ç¦»åº¦é‡
- [ ] è°ƒæ•´ M (connectivity) å‚æ•°
- [ ] è°ƒæ•´ ef (expansion) å‚æ•°
- [ ] ä½¿ç”¨é‡åŒ–ï¼ˆf16/i8/PQï¼‰
- [ ] å¯ç”¨é¢„å–

### æ•°æ®ç»“æ„ä¼˜åŒ–
- [ ] ä½¿ç”¨ SoA å¸ƒå±€
- [ ] ç¼“å­˜è¡Œå¯¹é½ï¼ˆ64å­—èŠ‚ï¼‰
- [ ] å†…å­˜æ± 
- [ ] å‹ç¼©å­˜å‚¨

### å¹¶å‘ä¼˜åŒ–
- [ ] æ‰¹é‡æ“ä½œ
- [ ] ç»†ç²’åº¦é”
- [ ] æ— é”æ•°æ®ç»“æ„
- [ ] OpenMP å¹¶è¡ŒåŒ–

### ç›‘æ§å’Œåˆ†æ
- [ ] ä½¿ç”¨ perf åˆ†æçƒ­ç‚¹
- [ ] ä½¿ç”¨ Valgrind æ£€æµ‹å†…å­˜æ³„æ¼
- [ ] ä½¿ç”¨ Flame Graph å¯è§†åŒ–
- [ ] å®šæœŸè¿è¡ŒåŸºå‡†æµ‹è¯•

---

## 10. æ€»ç»“

æ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ª**ç³»ç»ŸåŒ–çš„å·¥ç¨‹**ï¼š

1. **æµ‹é‡**ï¼šä½¿ç”¨å·¥å…·æ‰¾å‡ºç“¶é¢ˆ
2. **åˆ†æ**ï¼šç†è§£æ ¹æœ¬åŸå› 
3. **ä¼˜åŒ–**ï¼šé’ˆå¯¹æ€§å®æ–½ä¼˜åŒ–
4. **éªŒè¯**ï¼šé‡åŒ–ä¼˜åŒ–æ•ˆæœ
5. **è¿­ä»£**ï¼šæŒç»­æ”¹è¿›

è®°ä½ï¼š**è¿‡æ—©ä¼˜åŒ–æ˜¯ä¸‡æ¶ä¹‹æº**ï¼Œä½†åœ¨äº†è§£ç“¶é¢ˆåï¼Œä¼˜åŒ–æ˜¯å¿…éœ€çš„ï¼

---

**ä¸‹ä¸€æ­¥**ï¼šé€‰æ‹©ä¸€ä¸ªå®é™…é¡¹ç›®ï¼Œåº”ç”¨è¿™äº›ä¼˜åŒ–æŠ€å·§ï¼
