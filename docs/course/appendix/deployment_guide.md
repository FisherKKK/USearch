# USearch ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—
## Production Deployment Guide

---

## ğŸ“š ç›®å½•

1. [éƒ¨ç½²æ¶æ„](#éƒ¨ç½²æ¶æ„)
2. [å•æœºéƒ¨ç½²](#å•æœºéƒ¨ç½²)
3. [åˆ†å¸ƒå¼éƒ¨ç½²](#åˆ†å¸ƒå¼éƒ¨ç½²)
4. [äº‘åŸç”Ÿéƒ¨ç½²](#äº‘åŸç”Ÿéƒ¨ç½²)
5. [æ€§èƒ½è°ƒä¼˜](#æ€§èƒ½è°ƒä¼˜)
6. [ç›‘æ§å’Œå‘Šè­¦](#ç›‘æ§å’Œå‘Šè­¦)
7. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
8. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## 1. éƒ¨ç½²æ¶æ„

### 1.1 å•æœºæ¶æ„

**é€‚ç”¨åœºæ™¯**ï¼š
- å‘é‡æ•°é‡ < 1000 ä¸‡
- QPS < 1000
- å¿«é€ŸåŸå‹å¼€å‘

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Application Server    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  USearch Index    â”‚  â”‚
â”‚  â”‚  (Single Process) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 ä¸»ä»æ¶æ„

**é€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦é«˜å¯ç”¨
- è¯»å†™åˆ†ç¦»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â”€â”€â”€â”€â–º â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” (Read)
     â”‚       â”‚   Slave 1    â”‚
     â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â”€â”€â”€â”€â–º â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” (Read)
     â”‚       â”‚   Slave 2    â”‚
     â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â””â”€â”€â”€â”€â”€â–º â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” (Write)
             â”‚    Master    â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 åˆ†å¸ƒå¼é›†ç¾¤æ¶æ„

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤§è§„æ¨¡æ•°æ®ï¼ˆ> 1 äº¿å‘é‡ï¼‰
- é«˜ååé‡ï¼ˆ> 10,000 QPSï¼‰
- éœ€è¦æ°´å¹³æ‰©å±•

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Load      â”‚
                    â”‚  Balancer   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ Shard 1 â”‚      â”‚  Shard 2  â”‚     â”‚ Shard 3 â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Node A  â”‚      â”‚ Node B    â”‚     â”‚ Node C  â”‚
    â”‚ Node A' â”‚      â”‚ Node B'   â”‚     â”‚ Node C' â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. å•æœºéƒ¨ç½²

### 2.1 ç³»ç»Ÿè¦æ±‚

**æœ€ä½é…ç½®**ï¼š
- CPU: 4 æ ¸
- å†…å­˜: 8 GB
- å­˜å‚¨: 100 GB SSD
- OS: Linux (Ubuntu 20.04+, CentOS 8+)

**æ¨èé…ç½®**ï¼š
- CPU: 16 æ ¸ï¼ˆæ”¯æŒ AVX2ï¼‰
- å†…å­˜: 64 GB
- å­˜å‚¨: 1 TB NVMe SSD
- OS: Linux (Ubuntu 22.04 LTS)

### 2.2 å®‰è£…æ­¥éª¤

#### ä»æºç ç¼–è¯‘

```bash
# 1. å®‰è£…ä¾èµ–
sudo apt-get update
sudo apt-get install -y \
    build-essential \
    cmake \
    git \
    libgomp1

# 2. å…‹éš†ä»“åº“
git clone https://github.com/unum-cloud/usearch.git
cd usearch

# 3. ç¼–è¯‘
mkdir build && cd build
cmake .. \
    -DCMAKE_BUILD_TYPE=Release \
    -DUSEARCH_USE_OPENMP=ON \
    -DUSEARCH_USE_SIMSIMD=ON
make -j$(nproc)

# 4. å®‰è£…
sudo make install
```

#### Docker éƒ¨ç½²

```dockerfile
FROM ubuntu:22.04

# å®‰è£…ä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    git \
    libgomp1

# å¤åˆ¶æºç 
COPY . /usearch
WORKDIR /usearch

# ç¼–è¯‘
RUN mkdir build && cd build && \
    cmake .. -DCMAKE_BUILD_TYPE=Release \
    -DUSEARCH_USE_OPENMP=ON && \
    make -j$(nproc) && \
    make install

# è¿è¡Œ
CMD ["./usearch_server"]
```

æ„å»ºå’Œè¿è¡Œï¼š
```bash
docker build -t usearch:latest .
docker run -p 8080:8080 -v /data:/data usearch:latest
```

### 2.3 é…ç½®æ–‡ä»¶

åˆ›å»ºé…ç½®æ–‡ä»¶ `config.json`ï¼š

```json
{
  "index": {
    "dimensions": 768,
    "metric": "cos",
    "dtype": "f32",
    "connectivity": 16,
    "expansion": 64
  },
  "server": {
    "host": "0.0.0.0",
    "port": 8080,
    "threads": 8
  },
  "storage": {
    "index_path": "/data/index.usearch",
    "auto_save": true,
    "save_interval": 300
  }
}
```

### 2.4 æœåŠ¡ç®¡ç†

#### Systemd æœåŠ¡

åˆ›å»º `/etc/systemd/system/usearch.service`ï¼š

```ini
[Unit]
Description=USearch Vector Search Service
After=network.target

[Service]
Type=simple
User=usearch
WorkingDirectory=/opt/usearch
ExecStart=/opt/usearch/usearch_server --config /etc/usearch/config.json
Restart=always
RestartSec=10

# èµ„æºé™åˆ¶
LimitNOFILE=65536
LimitNPROC=4096

[Install]
WantedBy=multi-user.target
```

å¯ç”¨æœåŠ¡ï¼š
```bash
sudo systemctl daemon-reload
sudo systemctl enable usearch
sudo systemctl start usearch
sudo systemctl status usearch
```

---

## 3. åˆ†å¸ƒå¼éƒ¨ç½²

### 3.1 éƒ¨ç½²æ¶æ„è®¾è®¡

**ç¤ºä¾‹ï¼š4 èŠ‚ç‚¹é›†ç¾¤**

```yaml
# docker-compose.yml
version: '3.8'

services:
  # è´Ÿè½½å‡è¡¡å™¨
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - shard1
      - shard2
      - shard3
      - shard4

  # åˆ†ç‰‡èŠ‚ç‚¹
  shard1:
    image: usearch:latest
    environment:
      - SHARD_ID=0
      - SHARD_COUNT=4
    volumes:
      - shard1_data:/data

  shard2:
    image: usearch:latest
    environment:
      - SHARD_ID=1
      - SHARD_COUNT=4
    volumes:
      - shard2_data:/data

  shard3:
    image = usearch:latest
    environment:
      - SHARD_ID=2
      - SHARD_COUNT=4
    volumes:
      - shard3_data:/data

  shard4:
    image: usearch:latest
    environment:
      - SHARD_ID=3
      - SHARD_COUNT=4
    volumes:
      - shard4_data:/data

volumes:
  shard1_data:
  shard2_data:
  shard3_data:
  shard4_data:
```

### 3.2 Nginx é…ç½®

```nginx
# nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream usearch_cluster {
        least_conn;

        server shard1:8080 weight=1;
        server shard2:8080 weight=1;
        server shard3:8080 weight=1;
        server shard4:8080 weight=1;

        # å¥åº·æ£€æŸ¥
        check interval=3000 rise=2 fall=3 timeout=1000;
    }

    server {
        listen 80;

        location / {
            proxy_pass http://usearch_cluster;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;

            # è¶…æ—¶è®¾ç½®
            proxy_connect_timeout 5s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }

        # å¥åº·æ£€æŸ¥ç«¯ç‚¹
        location /health {
            access_log off;
            return 200 "OK\n";
            add_header Content-Type text/plain;
        }
    }
}
```

### 3.3 å¯åŠ¨é›†ç¾¤

```bash
# å¯åŠ¨æ‰€æœ‰èŠ‚ç‚¹
docker-compose up -d

# æ£€æŸ¥çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# æ‰©å±•åˆ†ç‰‡
docker-compose up -d --scale shard=8
```

---

## 4. äº‘åŸç”Ÿéƒ¨ç½²

### 4.1 Kubernetes éƒ¨ç½²

#### Deployment é…ç½®

```yaml
# usearch-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: usearch-shard
  labels:
    app: usearch
spec:
  replicas: 4
  selector:
    matchLabels:
      app: usearch
  template:
    metadata:
      labels:
        app: usearch
    spec:
      containers:
      - name: usearch
        image: usearch:latest
        ports:
        - containerPort: 8080
        env:
        - name: SHARD_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['shard-id']
        - name: SHARD_COUNT
          value: "4"
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
        volumeMounts:
        - name: data
          mountPath: /data
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
      volumes:
      - name: data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: usearch-service
spec:
  selector:
    app: usearch
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: LoadBalancer
```

åº”ç”¨é…ç½®ï¼š
```bash
kubectl apply -f usearch-deployment.yaml
kubectl get pods
kubectl get svc
```

### 4.2 Helm Chart

åˆ›å»º Chart ç»“æ„ï¼š
```
usearch-chart/
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ values.yaml
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â””â”€â”€ ingress.yaml
```

**values.yaml**ï¼š
```yaml
replicaCount: 4

image:
  repository: usearch
  tag: latest
  pullPolicy: IfNotPresent

service:
  type: LoadBalancer
  port: 80

resources:
  limits:
    cpu: 4
    memory: 8Gi
  requests:
    cpu: 2
    memory: 4Gi

config:
  dimensions: 768
  metric: cos
  connectivity: 16
  expansion: 64
```

éƒ¨ç½²ï¼š
```bash
helm install usearch ./usearch-chart
helm upgrade usearch ./usearch-chart
```

---

## 5. æ€§èƒ½è°ƒä¼˜

### 5.1 ç³»ç»Ÿçº§ä¼˜åŒ–

#### å†…æ ¸å‚æ•°è°ƒä¼˜

```bash
# /etc/sysctl.conf

# ç½‘ç»œä¼˜åŒ–
net.core.somaxconn = 65535
net.ipv4.tcp_max_syn_backlog = 8192
net.ipv4.tcp_tw_reuse = 1
net.ipv4.ip_local_port_range = 10000 65535

# å†…å­˜ä¼˜åŒ–
vm.swappiness = 1
vm.dirty_ratio = 15
vm.dirty_background_ratio = 5

# æ–‡ä»¶æè¿°ç¬¦
fs.file-max = 2097152
```

åº”ç”¨ï¼š
```bash
sudo sysctl -p
```

#### æ–‡ä»¶æè¿°ç¬¦é™åˆ¶

```bash
# /etc/security/limits.conf
usearch soft nofile 65536
usearch hard nofile 65536
usearch soft nproc 4096
usearch hard nproc 4096
```

### 5.2 åº”ç”¨çº§ä¼˜åŒ–

#### é…ç½®è°ƒä¼˜

```cpp
// é«˜æ€§èƒ½é…ç½®
index_dense_config_t config;

// è¿æ¥æ€§
config.connectivity = 16;      // å¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦
config.expansion = 64;         // æœç´¢èŒƒå›´

// å¹¶å‘
config.multi = true;           // å¯ç”¨å¤šçº¿ç¨‹

// å¯¹é½
config.alignment = 64;         // ç¼“å­˜è¡Œå¯¹é½

// åˆå§‹åŒ–
index.init(dimensions, metric_kind_t::cos_k, scalar_kind_t::f32_k, config);
```

#### å†…å­˜ä¼˜åŒ–

```cpp
// é¢„åˆ†é…
index.reserve(10000000);  // 1000ä¸‡å‘é‡

// ä½¿ç”¨é‡åŒ–
index.init(dimensions,
           metric_kind_t::cos_k,
           scalar_kind_t::f16_k);  // èŠ‚çœ50%å†…å­˜
```

### 5.3 æ‰¹å¤„ç†ä¼˜åŒ–

```cpp
// âŒ é€ä¸ªå¤„ç†
for (std::size_t i = 0; i < n; ++i) {
    index.add(keys[i], vectors + i * d);
}

// âœ… æ‰¹å¤„ç†
index.add(keys, vectors, n);

// âœ… å¹¶è¡Œæ‰¹å¤„ç†
#pragma omp parallel for schedule(dynamic)
for (std::size_t batch = 0; batch < n_batches; ++batch) {
    std::size_t start = batch * batch_size;
    std::size_t end = std::min(start + batch_size, n);
    index.add(keys + start, vectors + start * d, end - start);
}
```

---

## 6. ç›‘æ§å’Œå‘Šè­¦

### 6.1 Prometheus ç›‘æ§

#### å¯¼å‡ºæŒ‡æ ‡

```cpp
#include <prometheus/registry.h>
#include <prometheus/exposer.h>

class MonitoredIndex {
    index_dense_gt<float, std::uint32_t> index_;

    // Prometheus æŒ‡æ ‡
    prometheus::Counter& query_counter_;
    prometheus::Histogram& query_latency_;
    prometheus::Gauge& index_size_;

public:
    MonitoredIndex()
        : query_counter_(prometheus::BuildCounter()
            .Name("usearch_queries_total")
            .Register(prometheus::DefaultRegistry())
            .Add({})),
          query_latency_(prometheus::BuildHistogram()
            .Name("usearch_query_latency_seconds")
            .Register(prometheus::DefaultRegistry())
            .Add({})),
          index_size_(prometheus::BuildGauge()
            .Name("usearch_index_size")
            .Register(prometheus::DefaultRegistry())
            .Add({}))
    {}

    std::vector<result_t> search(float const* query, std::size_t k) {
        auto start = std::chrono::system_clock::now();

        auto results = index_.search(query, k);

        auto end = std::chrono::system_clock::now();
        auto duration = std::chrono::duration<double>(end - start).count();

        // è®°å½•æŒ‡æ ‡
        query_counter_.Increment();
        query_latency_.Observe(duration);
        index_size_.Set(index_.size());

        return results;
    }
};
```

#### Prometheus é…ç½®

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'usearch'
    static_configs:
      - targets: ['localhost:8080']
    metrics_path: '/metrics'
```

### 6.2 Grafana ä»ªè¡¨æ¿

#### å…³é”®æŒ‡æ ‡

**æŸ¥è¯¢æŒ‡æ ‡**ï¼š
- `rate(usearch_queries_total[5m])` - QPS
- `histogram_quantile(0.99, usearch_query_latency_seconds)` - P99 å»¶è¿Ÿ
- `histogram_quantile(0.95, usearch_query_latency_seconds)` - P95 å»¶è¿Ÿ

**ç´¢å¼•æŒ‡æ ‡**ï¼š
- `usearch_index_size` - ç´¢å¼•å¤§å°
- `usearch_memory_usage_bytes` - å†…å­˜ä½¿ç”¨
- `usearch_disk_usage_bytes` - ç£ç›˜ä½¿ç”¨

**ç³»ç»ŸæŒ‡æ ‡**ï¼š
- `rate(process_cpu_seconds_total[5m])` - CPU ä½¿ç”¨ç‡
- `process_resident_memory_bytes` - å†…å­˜ä½¿ç”¨

### 6.3 å‘Šè­¦è§„åˆ™

```yaml
# alerting.yml
groups:
  - name: usearch_alerts
    interval: 30s
    rules:
      # é«˜å»¶è¿Ÿå‘Šè­¦
      - alert: HighSearchLatency
        expr: histogram_quantile(0.99, usearch_query_latency_seconds) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High search latency detected"
          description: "P99 latency is {{ $value }}s"

      # ä½ QPS å‘Šè­¦
      - alert: LowQueryRate
        expr: rate(usearch_queries_total[5m]) < 100
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low query rate"
          description: "QPS is {{ $value }}"

      # å†…å­˜ä½¿ç”¨å‘Šè­¦
      - alert: HighMemoryUsage
        expr: usearch_memory_usage_bytes / node_memory_MemTotal_bytes > 0.9
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value }}%"
```

---

## 7. æ•…éšœæ’æŸ¥

### 7.1 å¸¸è§é—®é¢˜

#### é—®é¢˜ 1ï¼šæœç´¢æ…¢

**ç—‡çŠ¶**ï¼šP99 å»¶è¿Ÿ > 100ms

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥ CPU ä½¿ç”¨
top -p $(pidof usearch_server)

# æ£€æŸ¥çº¿ç¨‹æ•°
ps -eLf | grep usearch_server | wc -l

# æ£€æŸ¥ç´¢å¼•å¤§å°
curl http://localhost:8080/stats
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å¢åŠ  expansion å‚æ•°
2. å¯ç”¨å¤šçº¿ç¨‹
3. ä½¿ç”¨é‡åŒ–ï¼ˆf16/i8ï¼‰
4. æ·»åŠ ç¼“å­˜

#### é—®é¢˜ 2ï¼šå†…å­˜ä¸è¶³

**ç—‡çŠ¶**ï¼šOOM é”™è¯¯

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
free -h

# æ£€æŸ¥è¿›ç¨‹å†…å­˜
cat /proc/$(pidof usearch_server)/status | grep VmRSS

# ä½¿ç”¨ valgrind æ£€æŸ¥æ³„æ¼
valgrind --leak-check=full ./usearch_server
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨ f16 é‡åŒ–
2. å¯ç”¨ swap
3. åˆ†ç‰‡ç´¢å¼•
4. å¢åŠ ç‰©ç†å†…å­˜

#### é—®é¢˜ 3ï¼šæ„å»ºç´¢å¼•æ…¢

**ç—‡çŠ¶**ï¼šæ„å»º 1000 ä¸‡å‘é‡éœ€è¦ > 1 å°æ—¶

**è§£å†³æ–¹æ¡ˆ**ï¼š
```cpp
// æ‰¹é‡æ·»åŠ ï¼ˆå…³é”®ï¼‰
index.add(keys, vectors, n);

// å¹¶è¡Œæ„å»º
#pragma omp parallel for
for (std::size_t i = 0; i < n_batches; ++i) {
    // æ‰¹é‡æ·»åŠ 
}

// é™ä½ ef_construction
config.expansion = 32;  // é™ä½ä»¥æé«˜é€Ÿåº¦
```

### 7.2 è°ƒè¯•å·¥å…·

#### æ€§èƒ½åˆ†æ

```bash
# CPU æ€§èƒ½åˆ†æ
perf record -g -p $(pidof usearch_server)
perf report

# ç«ç„°å›¾
git clone https://github.com/brendangregg/FlameGraph
perf script | FlameGraph/stackcollapse-perf.pl | \
    FlameGraph/flamegraph.pl > flamegraph.svg
```

#### å†…å­˜åˆ†æ

```bash
# å†…å­˜æ˜ å°„
pmap -x $(pidof usearch_server)

# å †åˆ†æ
jmap -heap $(pidof usearch_server)

# å†…å­˜æ³„æ¼æ£€æµ‹
valgrind --leak-check=full --show-leak-kinds=all \
         --track-origins=yes ./usearch_server
```

---

## 8. æœ€ä½³å®è·µ

### 8.1 å®¹é‡è§„åˆ’

**ä¼°ç®—å…¬å¼**ï¼š

```
å†…å­˜éœ€æ±‚ = å‘é‡æ•°é‡ Ã— ç»´åº¦ Ã— å­—èŠ‚æ•° Ã— (1 + overhead)

ç¤ºä¾‹ï¼š1000ä¸‡ 768ç»´å‘é‡
- f32: 10M Ã— 768 Ã— 4 Ã— 1.5 = ~46 GB
- f16: 10M Ã— 768 Ã— 2 Ã— 1.5 = ~23 GB
- i8:  10M Ã— 768 Ã— 1 Ã— 1.5 = ~11 GB
```

**QPS ä¼°ç®—**ï¼š

```
å•èŠ‚ç‚¹ QPS = æ ¸å¿ƒæ•° / å¹³å‡å»¶è¿Ÿ(ç§’)

ç¤ºä¾‹ï¼š16 æ ¸ï¼Œå¹³å‡å»¶è¿Ÿ 5ms
QPS = 16 / 0.005 = 3200
```

### 8.2 å¤‡ä»½ç­–ç•¥

#### å¢é‡å¤‡ä»½

```bash
#!/bin/bash
# backup.sh

BACKUP_DIR="/backup/usearch"
INDEX_PATH="/data/index.usearch"
DATE=$(date +%Y%m%d_%H%M%S)

# åˆ›å»ºå¿«ç…§
cp $INDEX_PATH $BACKUP_DIR/index_$DATE.usearch

# å‹ç¼©
gzip $BACKUP_DIR/index_$DATE.usearch

# ä¸Šä¼ åˆ° S3
aws s3 cp $BACKUP_DIR/index_$DATE.usearch.gz \
    s3://backups/usearch/

# æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘7å¤©ï¼‰
find $BACKUP_DIR -name "index_*.usearch.gz" -mtime +7 -delete
```

#### å®šæ—¶ä»»åŠ¡

```bash
# æ·»åŠ åˆ° crontab
0 */6 * * * /opt/scripts/backup.sh
```

### 8.3 å®‰å…¨é…ç½®

#### ç½‘ç»œéš”ç¦»

```yaml
# ä»…å…è®¸å†…ç½‘è®¿é—®
# iptables
-A INPUT -p tcp --dport 8080 -s 10.0.0.0/8 -j ACCEPT
-A INPUT -p tcp --dport 8080 -j DROP
```

#### è®¤è¯

```cpp
// æ·»åŠ  API å¯†é’¥éªŒè¯
bool authenticate(std::string const& api_key) {
    static const std::set<std::string> valid_keys = {
        "key1", "key2", "key3"
    };
    return valid_keys.count(api_key) > 0;
}
```

### 8.4 ç‰ˆæœ¬å‡çº§

**æ»šåŠ¨å‡çº§**ï¼š

```bash
# 1. æ‹‰å–æ–°é•œåƒ
docker pull usearch:v2.0

# 2. é€ä¸ªå‡çº§èŠ‚ç‚¹
for shard in shard1 shard2 shard3 shard4; do
    docker-compose up -d --no-deps --force-recreate $shard
    sleep 30  # ç­‰å¾…èŠ‚ç‚¹å°±ç»ª
done

# 3. éªŒè¯
curl http://localhost/health
```

---

## 9. æ£€æŸ¥æ¸…å•

### éƒ¨ç½²å‰æ£€æŸ¥

- [ ] ç¡¬ä»¶èµ„æºæ»¡è¶³è¦æ±‚
- [ ] æ“ä½œç³»ç»Ÿç‰ˆæœ¬å…¼å®¹
- [ ] ä¾èµ–åº“å·²å®‰è£…
- [ ] ç½‘ç»œé…ç½®æ­£ç¡®
- [ ] é˜²ç«å¢™è§„åˆ™å·²è®¾ç½®
- [ ] å­˜å‚¨ç©ºé—´å……è¶³
- [ ] å¤‡ä»½ç­–ç•¥å·²åˆ¶å®š

### éƒ¨ç½²åæ£€æŸ¥

- [ ] æœåŠ¡æ­£å¸¸å¯åŠ¨
- [ ] å¥åº·æ£€æŸ¥é€šè¿‡
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ
- [ ] ç›‘æ§é…ç½®å®Œæˆ
- [ ] å‘Šè­¦è§„åˆ™è®¾ç½®
- [ ] æ—¥å¿—æ­£å¸¸è¾“å‡º
- [ ] å¤‡ä»½ä»»åŠ¡è¿è¡Œ

### è¿ç»´æ£€æŸ¥

- [ ] æ¯æ—¥ï¼šæ£€æŸ¥ç³»ç»Ÿæ—¥å¿—
- [ ] æ¯å‘¨ï¼šæ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
- [ ] æ¯æœˆï¼šæ£€æŸ¥å¤‡ä»½å®Œæ•´æ€§
- [ ] æ¯å­£åº¦ï¼šå®¹é‡è§„åˆ’è¯„ä¼°
- [ ] æ¯å¹´ï¼šç¾éš¾æ¢å¤æ¼”ç»ƒ

---

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-01-24
