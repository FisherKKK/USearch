# å®æˆ˜é¡¹ç›®æŒ‡å—

## é¡¹ç›®æ¦‚è¿°

æœ¬æŒ‡å—æä¾›ä¸‰ä¸ªä¸åŒéš¾åº¦çš„å®æˆ˜é¡¹ç›®ï¼Œå¸®åŠ©ä½ å°†14å¤©è¯¾ç¨‹ä¸­å­¦åˆ°çš„çŸ¥è¯†åº”ç”¨åˆ°å®é™…åœºæ™¯ä¸­ã€‚

---

## ğŸ¥‰ é¡¹ç›® 1ï¼šåŸºç¡€å‘é‡æœç´¢å¼•æ“ï¼ˆåˆçº§ï¼‰

### ç›®æ ‡
å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„å‘é‡æœç´¢å¼•æ“ï¼Œæ”¯æŒåŸºæœ¬çš„æ·»åŠ å’Œæœç´¢åŠŸèƒ½ã€‚

### åŠŸèƒ½è¦æ±‚

#### å¿…é¡»å®ç°
- [x] æ”¯æŒæ·»åŠ å‘é‡ï¼ˆf32ï¼‰
- [x] æ”¯æŒæœç´¢æœ€è¿‘é‚»ï¼ˆæš´åŠ›æœç´¢ï¼‰
- [x] æ”¯æŒè‡³å°‘ä¸¤ç§è·ç¦»åº¦é‡ï¼ˆL2ã€Cosineï¼‰
- [x] æä¾›å‘½ä»¤è¡Œç•Œé¢

#### åŠ åˆ†é¡¹
- [ ] ä½¿ç”¨ HNSW ç®—æ³•ï¼ˆè€Œéæš´åŠ›æœç´¢ï¼‰
- [ ] æ”¯æŒä»æ–‡ä»¶åŠ è½½æ•°æ®
- [ ] æä¾›æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] æ”¯æŒåˆ é™¤å‘é‡

### æŠ€æœ¯æ ˆ
- C++17
- STLï¼ˆæ ‡å‡†åº“ï¼‰
- å¯é€‰ï¼šUSearchï¼ˆå‚è€ƒå®ç°ï¼‰

### å®ç°æ­¥éª¤

#### ç¬¬1æ­¥ï¼šè®¾è®¡æ•°æ®ç»“æ„
```cpp
class SimpleVectorSearch {
    std::vector<std::vector<float>> vectors_;
    std::vector<std::size_t> keys_;

public:
    void add(std::size_t key, const std::vector<float>& vector);
    std::vector<std::pair<std::size_t, float>> search(
        const std::vector<float>& query,
        std::size_t k,
        const std::string& metric = "l2"
    );
};
```

#### ç¬¬2æ­¥ï¼šå®ç°è·ç¦»è®¡ç®—
```cpp
float l2_distance(const float* a, const float* b, std::size_t dims);
float cosine_distance(const float* a, const float* b, std::size_t dims);
```

#### ç¬¬3æ­¥ï¼šå®ç°æœç´¢
```cpp
std::vector<std::pair<std::size_t, float>> SimpleVectorSearch::search(
    const std::vector<float>& query,
    std::size_t k,
    const std::string& metric
) {
    std::vector<std::pair<std::size_t, float>> distances;

    // è®¡ç®—æ‰€æœ‰è·ç¦»
    for (std::size_t i = 0; i < vectors_.size(); ++i) {
        float dist;
        if (metric == "l2") {
            dist = l2_distance(query.data(), vectors_[i].data(), query.size());
        } else if (metric == "cosine") {
            dist = cosine_distance(query.data(), vectors_[i].data(), query.size());
        }
        distances.emplace_back(keys_[i], dist);
    }

    // æ’åºå¹¶è¿”å› top-k
    std::partial_sort(distances.begin(), distances.begin() + k, distances.end(),
        [](auto& a, auto& b) { return a.second < b.second; });

    return std::vector<std::pair<std::size_t, float>>(
        distances.begin(), distances.begin() + k
    );
}
```

#### ç¬¬4æ­¥ï¼šå‘½ä»¤è¡Œç•Œé¢
```cpp
int main() {
    SimpleVectorSearch engine;

    // ç¤ºä¾‹ï¼šæ·»åŠ å‘é‡
    for (int i = 0; i < 1000; ++i) {
        std::vector<float> vec(128);
        // ... å¡«å……éšæœºæ•°æ® ...
        engine.add(i, vec);
    }

    // æœç´¢
    std::vector<float> query(128);
    // ... å¡«å……æŸ¥è¯¢ ...

    auto results = engine.search(query, 10, "l2");

    for (auto [key, dist] : results) {
        std::cout << key << ": " << dist << "\n";
    }

    return 0;
}
```

### éªŒæ”¶æ ‡å‡†
- [ ] èƒ½æ­£ç¡®æ·»åŠ å’Œæœç´¢ 1000 ä¸ª 128 ç»´å‘é‡
- [ ] æœç´¢ç»“æœå‡†ç¡®ï¼ˆä¸æš´åŠ›æœç´¢å¯¹æ¯”ï¼‰
- [ ] æä¾› Makefile æˆ– CMakeLists.txt
- [ ] åŒ…å« README å’Œä½¿ç”¨è¯´æ˜

---

## ğŸ¥ˆ é¡¹ç›® 2ï¼šå›¾åƒç›¸ä¼¼åº¦æœç´¢ç³»ç»Ÿï¼ˆä¸­çº§ï¼‰

### ç›®æ ‡
æ„å»ºä¸€ä¸ªå®Œæ•´çš„å›¾åƒæœç´¢ç³»ç»Ÿï¼Œæ”¯æŒä¸Šä¼ å›¾ç‰‡å¹¶è¿”å›ç›¸ä¼¼å›¾ç‰‡ã€‚

### åŠŸèƒ½è¦æ±‚

#### å¿…é¡»å®ç°
- [x] å›¾åƒç‰¹å¾æå–ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼‰
- [x] å‘é‡ç´¢å¼•æ„å»ºï¼ˆä½¿ç”¨ USearchï¼‰
- [x] ç›¸ä¼¼å›¾ç‰‡æœç´¢
- [x] Web ç•Œé¢ï¼ˆFlask/FastAPIï¼‰

#### åŠ åˆ†é¡¹
- [ ] æ”¯æŒå¤šç§ç‰¹å¾æå–æ–¹æ³•
- [ ] å›¾åƒé¢„å¤„ç†å’Œå¢å¼º
- [ ] ç»“æœå¯è§†åŒ–
- [ ] æ€§èƒ½ä¼˜åŒ–ï¼ˆç¼“å­˜ã€æ‰¹å¤„ç†ï¼‰

### æŠ€æœ¯æ ˆ
- Python 3.8+
- PyTorch/TensorFlowï¼ˆç‰¹å¾æå–ï¼‰
- USearchï¼ˆå‘é‡æœç´¢ï¼‰
- Flask/FastAPIï¼ˆWebæ¡†æ¶ï¼‰
- HTML/CSS/JavaScriptï¼ˆå‰ç«¯ï¼‰

### å®ç°æ­¥éª¤

#### ç¬¬1æ­¥ï¼šç‰¹å¾æå–
```python
import torch
import torchvision.models as models
from PIL import Image
from torchvision import transforms

class FeatureExtractor:
    def __init__(self):
        # ä½¿ç”¨é¢„è®­ç»ƒçš„ ResNet50
        self.model = models.resnet50(pretrained=True)
        self.model.eval()

        # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
        self.feature_extractor = torch.nn.Sequential(*list(self.model.children())[:-1])

        self.preprocess = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

    def extract(self, image_path):
        image = Image.open(image_path).convert('RGB')
        input_tensor = self.preprocess(image)
        input_batch = input_tensor.unsqueeze(0)

        with torch.no_grad():
            features = self.feature_extractor(input_batch)

        return features.squeeze().numpy()
```

#### ç¬¬2æ­¥ï¼šç´¢å¼•æ„å»º
```python
import usearch
import numpy as np
import glob

class ImageIndex:
    def __init__(self, dimension=2048):
        self.index = usearch.Index(
            ndim=dimension,
            metric='cos',
            dtype='f32'
        )
        self.extractor = FeatureExtractor()
        self.image_paths = []

    def build(self, image_dir):
        """ä»ç›®å½•æ„å»ºç´¢å¼•"""
        image_paths = glob.glob(f"{image_dir}/*.jpg")[:10000]

        print(f"ç´¢å¼• {len(image_paths)} å¼ å›¾ç‰‡...")

        batch_size = 32
        for i in range(0, len(image_paths), batch_size):
            batch_paths = image_paths[i:i+batch_size]
            features = []

            for path in batch_paths:
                feat = self.extractor.extract(path)
                features.append(feat)

            features_array = np.vstack(features).astype(np.float32)
            keys = np.arange(i, i + len(batch_paths), dtype=np.uint64)

            self.index.add(keys, features_array)
            self.image_paths.extend(batch_paths)

            if (i // batch_size + 1) % 10 == 0:
                print(f"  å·²å¤„ç† {i + len(batch_paths)} å¼ ")

        self.index.save("image_index.usearch")
        print("ç´¢å¼•å®Œæˆ")

    def search(self, query_image_path, k=10):
        """æœç´¢ç›¸ä¼¼å›¾ç‰‡"""
        query_feat = self.extractor.extract(query_image_path)
        results = self.index.search(query_feat.astype(np.float32), k)

        similar_images = []
        for key, distance in results:
            similar_images.append({
                'path': self.image_paths[key],
                'score': 1 - distance  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
            })

        return similar_images
```

#### ç¬¬3æ­¥ï¼šWeb API
```python
from flask import Flask, request, jsonify, send_file
import os

app = Flask(__name__)
index = None

@app.route('/build', methods=['POST'])
def build_index():
    """æ„å»ºç´¢å¼•"""
    data = request.json
    image_dir = data['image_dir']

    global index
    index = ImageIndex()
    index.build(image_dir)

    return jsonify({'status': 'ok', 'count': len(index.image_paths)})

@app.route('/search', methods=['POST'])
def search():
    """æœç´¢ç›¸ä¼¼å›¾ç‰‡"""
    if 'image' not in request.files:
        return jsonify({'error': 'No image uploaded'}), 400

    file = request.files['image']
    k = int(request.form.get('k', 10))

    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    temp_path = f'/tmp/{file.filename}'
    file.save(temp_path)

    # æœç´¢
    results = index.search(temp_path, k)

    # æ¸…ç†
    os.remove(temp_path)

    return jsonify({'results': results})

@app.route('/image/<path:filename>')
def get_image(filename):
    """è¿”å›å›¾ç‰‡"""
    return send_file(filename)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
```

#### ç¬¬4æ­¥ï¼šå‰ç«¯ç•Œé¢
```html
<!DOCTYPE html>
<html>
<head>
    <title>å›¾åƒæœç´¢</title>
    <style>
        .gallery {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
            gap: 16px;
            margin-top: 20px;
        }
        .image-card {
            border: 1px solid #ddd;
            padding: 8px;
            border-radius: 8px;
        }
        .image-card img {
            width: 100%;
            height: 200px;
            object-fit: cover;
        }
        .score {
            font-weight: bold;
            color: #007bff;
        }
    </style>
</head>
<body>
    <h1>å›¾åƒç›¸ä¼¼åº¦æœç´¢</h1>

    <div>
        <input type="file" id="queryImage" accept="image/*">
        <button onclick="search()">æœç´¢</button>
        <input type="number" id="k" value="10" min="1" max="100">
    </div>

    <div class="gallery" id="results"></div>

    <script>
        async function search() {
            const fileInput = document.getElementById('queryImage');
            const k = document.getElementById('k').value;

            if (!fileInput.files[0]) {
                alert('è¯·é€‰æ‹©å›¾ç‰‡');
                return;
            }

            const formData = new FormData();
            formData.append('image', fileInput.files[0]);
            formData.append('k', k);

            const response = await fetch('/search', {
                method: 'POST',
                body: formData
            });

            const data = await response.json();
            displayResults(data.results);
        }

        function displayResults(results) {
            const gallery = document.getElementById('results');
            gallery.innerHTML = '';

            results.forEach(result => {
                const card = document.createElement('div');
                card.className = 'image-card';
                card.innerHTML = `
                    <img src="/image/${result.path}" alt="result">
                    <div>ç›¸ä¼¼åº¦: <span class="score">${result.score.toFixed(3)}</span></div>
                `;
                gallery.appendChild(card);
            });
        }
    </script>
</body>
</html>
```

### éªŒæ”¶æ ‡å‡†
- [ ] èƒ½ç´¢å¼•è‡³å°‘ 1000 å¼ å›¾ç‰‡
- [ ] æœç´¢å“åº”æ—¶é—´ < 100ms
- [ ] Web ç•Œé¢å‹å¥½æ˜“ç”¨
- [ ] åŒ…å«å®Œæ•´çš„éƒ¨ç½²æ–‡æ¡£

---

## ğŸ¥‡ é¡¹ç›® 3ï¼šRAG çŸ¥è¯†é—®ç­”ç³»ç»Ÿï¼ˆé«˜çº§ï¼‰

### ç›®æ ‡
æ„å»ºä¸€ä¸ªåŸºäº RAG çš„çŸ¥è¯†é—®ç­”ç³»ç»Ÿï¼Œç»“åˆå‘é‡æ£€ç´¢å’Œ LLM ç”Ÿæˆã€‚

### åŠŸèƒ½è¦æ±‚

#### å¿…é¡»å®ç°
- [x] æ–‡æ¡£åˆ‡ç‰‡å’Œå‘é‡åŒ–
- [x] å‘é‡ç´¢å¼•å’Œæ£€ç´¢
- [x] LLM é›†æˆï¼ˆç”Ÿæˆç­”æ¡ˆï¼‰
- [x] API æ¥å£
- [x] æ€§èƒ½ä¼˜åŒ–

#### åŠ åˆ†é¡¹
- [ ] æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ˆPDFã€DOCXã€Markdownï¼‰
- [ ] å¤šè½®å¯¹è¯
- [ ] å¼•ç”¨æ¥æºæ ‡æ³¨
- [ ] è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€å¬å›ç‡ï¼‰

### æŠ€æœ¯æ ˆ
- Python 3.8+
- LangChain/LlamaIndex
- USearch æˆ–å…¶ä»–å‘é‡æ•°æ®åº“
- OpenAI API / æœ¬åœ° LLM
- FastAPI

### å®ç°æ­¥éª¤

#### ç¬¬1æ­¥ï¼šæ–‡æ¡£å¤„ç†
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer

class DocumentProcessor:
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            length=len
        )
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')

    def process(self, documents):
        """å¤„ç†æ–‡æ¡£å¹¶è¿”å›chunks"""
        all_chunks = []

        for doc in documents:
            chunks = self.text_splitter.split_text(doc['text'])
            for i, chunk in enumerate(chunks):
                all_chunks.append({
                    'text': chunk,
                    'source': doc['source'],
                    'chunk_id': i
                })

        return all_chunks

    def encode(self, chunks):
        """ç¼–ç chunks"""
        texts = [chunk['text'] for chunk in chunks]
        embeddings = self.encoder.encode(texts)
        return embeddings.astype(np.float32)
```

#### ç¬¬2æ­¥ï¼šå‘é‡å­˜å‚¨
```python
import usearch
import numpy as np
import pickle

class VectorStore:
    def __init__(self, dimension=384):
        self.index = usearch.Index(
            ndim=dimension,
            metric='cos',
            dtype='f32',
            connectivity=16,
            expansion=64
        )
        self.chunks = []

    def add_documents(self, chunks, embeddings):
        """æ·»åŠ æ–‡æ¡£chunks"""
        keys = np.arange(len(self.chunks), len(self.chunks) + len(chunks), dtype=np.uint64)

        self.index.add(keys, embeddings)
        self.chunks.extend(chunks)

    def save(self, path):
        """ä¿å­˜ç´¢å¼•å’Œchunks"""
        self.index.save(f"{path}.usearch")
        with open(f"{path}.chunks", 'wb') as f:
            pickle.dump(self.chunks, f)

    def load(self, path):
        """åŠ è½½ç´¢å¼•å’Œchunks"""
        self.index.load(f"{path}.usearch")
        with open(f"{path}.chunks", 'rb') as f:
            self.chunks = pickle.load(f)

    def search(self, query_embedding, k=5):
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        results = self.index.search(query_embedding.astype(np.float32), k)

        retrieved_chunks = []
        for key, distance in results:
            retrieved_chunks.append({
                'chunk': self.chunks[key],
                'score': 1 - distance
            })

        return retrieved_chunks
```

#### ç¬¬3æ­¥ï¼šRAG ç³»ç»Ÿ
```python
import openai
from typing import List, Dict

class RAGSystem:
    def __init__(self, api_key=None):
        self.processor = DocumentProcessor()
        self.vector_store = VectorStore()

        if api_key:
            openai.api_key = api_key

    def index_documents(self, documents: List[Dict]):
        """ç´¢å¼•æ–‡æ¡£"""
        print("å¤„ç†æ–‡æ¡£...")
        chunks = self.processor.process(documents)

        print("ç¼–ç chunks...")
        embeddings = self.processor.encode(chunks)

        print("æ„å»ºç´¢å¼•...")
        self.vector_store.add_documents(chunks, embeddings)
        self.vector_store.save("knowledge_base")

        print(f"ç´¢å¼•å®Œæˆï¼Œå…± {len(chunks)} ä¸ªchunks")

    def query(self, question: str, k: int = 3) -> Dict:
        """æŸ¥è¯¢çŸ¥è¯†åº“"""
        # ç¼–ç é—®é¢˜
        question_embedding = self.processor.encoder.encode([question])[0]

        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_chunks = self.vector_store.search(question_embedding, k)

        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"ã€æ¥æº: {chunk['chunk']['source']}ã€‘\n{chunk['chunk']['text']}"
            for chunk in retrieved_chunks
        ])

        # ç”Ÿæˆç­”æ¡ˆ
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

ç­”æ¡ˆï¼š"""

        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )

        answer = response.choices[0].message.content

        return {
            'question': question,
            'answer': answer,
            'sources': [
                {
                    'text': chunk['chunk']['text'][:200] + "...",
                    'source': chunk['chunk']['source'],
                    'score': chunk['score']
                }
                for chunk in retrieved_chunks
            ]
        }
```

#### ç¬¬4æ­¥ï¼šFastAPI æ¥å£
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List

app = FastAPI(title="RAG Knowledge System")
rag = RAGSystem()

class Document(BaseModel):
    text: str
    source: str

class QueryRequest(BaseModel):
    question: str
    k: int = 3

@app.post("/index")
def index_documents(documents: List[Document]):
    """ç´¢å¼•æ–‡æ¡£"""
    try:
        docs = [doc.dict() for doc in documents]
        rag.index_documents(docs)
        return {"status": "ok", "indexed": len(docs)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/query")
def query(request: QueryRequest):
    """æŸ¥è¯¢çŸ¥è¯†åº“"""
    try:
        result = rag.query(request.question, request.k)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### éªŒæ”¶æ ‡å‡†
- [ ] èƒ½ç´¢å¼•è‡³å°‘ 1000 ä¸ªæ–‡æ¡£chunks
- [ ] ç«¯åˆ°ç«¯å»¶è¿Ÿ < 3 ç§’
- [ ] æä¾›æ¸…æ™°çš„APIæ–‡æ¡£
- [ ] åŒ…å«è¯„ä¼°æŒ‡æ ‡

---

## é¡¹ç›®è¯„ä¼°æ ‡å‡†

### ä»£ç è´¨é‡ (30åˆ†)
- ä»£ç ç»“æ„å’Œç»„ç»‡
- å‘½åè§„èŒƒ
- æ³¨é‡Šå’Œæ–‡æ¡£
- é”™è¯¯å¤„ç†

### åŠŸèƒ½å®Œæ•´æ€§ (40åˆ†)
- å¿…é¡»åŠŸèƒ½å…¨éƒ¨å®ç°
- åŠ åˆ†é¡¹å®ç°æƒ…å†µ
- åŠŸèƒ½çš„æ­£ç¡®æ€§

### æ€§èƒ½ (20åˆ†)
- å“åº”æ—¶é—´
- å†…å­˜ä½¿ç”¨
- å¯æ‰©å±•æ€§

### æ–‡æ¡£å’Œæ¼”ç¤º (10åˆ†)
- README å®Œæ•´æ€§
- ä½¿ç”¨è¯´æ˜
- æ¼”ç¤ºæ•ˆæœ

## æäº¤è¦æ±‚

1. **æºä»£ç **ï¼šGitHub ä»“åº“é“¾æ¥
2. **æ–‡æ¡£**ï¼šREADME.mdï¼ŒåŒ…æ‹¬å®‰è£…å’Œä½¿ç”¨è¯´æ˜
3. **æ¼”ç¤º**ï¼š5-10 åˆ†é’Ÿæ¼”ç¤ºè§†é¢‘æˆ–æˆªå›¾
4. **æŠ¥å‘Š**ï¼šç®€çŸ­çš„æŠ€æœ¯æŠ¥å‘Šï¼ˆ2-3é¡µï¼‰

## å¸¸è§é—®é¢˜

### Q: å¯ä»¥ä½¿ç”¨å…¶ä»–è¯­è¨€å—ï¼Ÿ
**A**: æ¨èä½¿ç”¨ C++ æˆ– Pythonï¼Œä½†ä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–è¯­è¨€ã€‚

### Q: å¯ä»¥ä½¿ç”¨ç°æˆçš„å‘é‡æ•°æ®åº“å—ï¼Ÿ
**A**: é¡¹ç›®1éœ€è¦è‡ªå·±å®ç°ï¼Œé¡¹ç›®2å’Œ3å¯ä»¥ä½¿ç”¨ USearchã€‚

### Q: å¦‚ä½•æµ‹è¯•æ€§èƒ½ï¼Ÿ
**A**: å¯ä»¥ä½¿ç”¨è¯¾ç¨‹ä¸­çš„æ€§èƒ½æµ‹è¯•ä»£ç ï¼Œæˆ–è‡ªè¡Œç¼–å†™åŸºå‡†æµ‹è¯•ã€‚

### Q: éœ€è¦å®Œæ•´çš„éƒ¨ç½²å—ï¼Ÿ
**A**: ä¸éœ€è¦ï¼Œæœ¬åœ°è¿è¡Œå³å¯ï¼Œä½†æä¾›éƒ¨ç½²è¯´æ˜ä¼šåŠ åˆ†ã€‚

## èµ„æº

- **USearch GitHub**: https://github.com/unum-cloud/usearch
- **ç¤ºä¾‹ä»£ç **: `examples/` ç›®å½•
- **æ•°æ®é›†**:
  - SIFT-1M: https://corpus-texmex.irisa.fr/
  - ImageNet: https://www.image-net.org/
  - Wikipedia: https://dumps.wikimedia.org/

ç¥ä½ é¡¹ç›®é¡ºåˆ©ï¼å¦‚æœ‰é—®é¢˜ï¼Œæ¬¢è¿åœ¨è¯¾ç¨‹ä»“åº“æ Issueã€‚
