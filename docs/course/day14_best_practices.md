# USearch æºç æ·±åº¦è§£æï¼šç¬¬14å¤©
## ç»¼åˆæ¡ˆä¾‹å’Œæœ€ä½³å®è·µ

---

## ğŸ“š ä»Šæ—¥å­¦ä¹ ç›®æ ‡

- ç»¼åˆè¿ç”¨å‰13å¤©æ‰€å­¦çŸ¥è¯†
- å­¦ä¹ çœŸå®åœºæ™¯çš„æœ€ä½³å®è·µ
- æŒæ¡ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŠ€å·§
- ç†è§£æ€§èƒ½è°ƒä¼˜çš„å®Œæ•´æµç¨‹
- æ„å»ºå®Œæ•´çš„å‘é‡æœç´¢åº”ç”¨

---

## 1. å®Œæ•´æ¡ˆä¾‹ï¼šRAG ç³»ç»Ÿ

### 1.1 åœºæ™¯æè¿°

**RAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿ**ï¼š

```
ç”¨æˆ·é—®é¢˜
    â†“
å‘é‡åŒ–ï¼ˆBERTï¼‰â†’ 768 ç»´å‘é‡
    â†“
USearch æœç´¢ â†’ Top-5 ç›¸å…³æ–‡æ¡£
    â†“
LLM ç”Ÿæˆç­”æ¡ˆ
```

### 1.2 å®Œæ•´å®ç°

**Python å®ç°**ï¼š

```python
import usearch
import numpy as np
from transformers import AutoTokenizer, AutoModel
import torch

class RAGSystem:
    def __init__(self, documents):
        # 1. åˆå§‹åŒ–æ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
        self.model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')

        # 2. åˆ›å»ºå‘é‡ç´¢å¼•
        self.index = usearch.Index(
            ndim=384,              # MiniLM ç»´åº¦
            metric='cos',           # ä½™å¼¦ç›¸ä¼¼åº¦
            dtype='f32',            # 32ä½æµ®ç‚¹
            connectivity=16,        # è¿æ¥æ•°
            expansion=64            # æœç´¢æ‰©å±•
        )

        # 3. ç´¢å¼•æ–‡æ¡£
        self.documents = documents
        self._index_documents()

    def _encode(self, texts):
        """å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡"""
        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')

        with torch.no_grad():
            outputs = self.model(**inputs)
            embeddings = outputs.last_hidden_state.mean(dim=1)

        return embeddings.cpu().numpy().astype(np.float32)

    def _index_documents(self):
        """ç´¢å¼•æ‰€æœ‰æ–‡æ¡£"""
        print(f"ç´¢å¼• {len(self.documents)} ä¸ªæ–‡æ¡£...")

        # æ‰¹é‡ç¼–ç 
        batch_size = 32
        embeddings = []
        for i in range(0, len(self.documents), batch_size):
            batch = self.documents[i:i+batch_size]
            batch_embeddings = self._encode(batch)
            embeddings.append(batch_embeddings)

        embeddings = np.vstack(embeddings)

        # æ·»åŠ åˆ°ç´¢å¼•
        keys = np.arange(len(self.documents), dtype=np.uint64)
        self.index.add(keys, embeddings)

        # ä¿å­˜ç´¢å¼•
        self.index.save("rag_index.usearch")
        print("ç´¢å¼•å®Œæˆå¹¶ä¿å­˜")

    def query(self, question, k=5):
        """æŸ¥è¯¢ç›¸å…³æ–‡æ¡£"""
        # 1. ç¼–ç é—®é¢˜
        question_embedding = self._encode([question])[0]

        # 2. æœç´¢
        results = self.index.search(question_embedding, k)

        # 3. è¿”å›æ–‡æ¡£
        relevant_docs = []
        for key, distance in results:
            doc = self.documents[key]
            score = 1 - distance  # ä½™å¼¦è·ç¦» â†’ ç›¸ä¼¼åº¦
            relevant_docs.append((doc, score))

        return relevant_docs

    def answer(self, question, k=3):
        """ç”Ÿæˆç­”æ¡ˆ"""
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self.query(question, k=k)

        # 2. æ„å»ºæç¤º
        context = "\n".join([f"- {doc}" for doc, _ in docs])
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

ç­”æ¡ˆï¼š"""

        # 3. è°ƒç”¨ LLMï¼ˆçœç•¥å®ç°ï¼‰
        # answer = call_llm(prompt)

        # è¿”å›ä¸Šä¸‹æ–‡ä¾›è°ƒè¯•
        return context, docs

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ–‡æ¡£é›†åˆ
    documents = [
        "Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”± Guido van Rossum åˆ›å»ºã€‚",
        "USearch æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å‘é‡æœç´¢å¼•æ“ã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ã€‚",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ã€‚",
        "HNSW æ˜¯ä¸€ç§é«˜æ•ˆçš„è¿‘ä¼¼æœ€è¿‘é‚»ç®—æ³•ã€‚",
        # ... æ›´å¤šæ–‡æ¡£
    ]

    # åˆ›å»º RAG ç³»ç»Ÿ
    rag = RAGSystem(documents)

    # æŸ¥è¯¢
    question = "ä»€ä¹ˆæ˜¯ USearchï¼Ÿ"
    context, relevant_docs = rag.answer(question)

    print(f"é—®é¢˜ï¼š{question}\n")
    print("ç›¸å…³æ–‡æ¡£ï¼š")
    for doc, score in relevant_docs:
        print(f"  [{score:.3f}] {doc}")
```

### 1.3 æ€§èƒ½ä¼˜åŒ–

**ä¼˜åŒ–1ï¼šæ‰¹é‡ç´¢å¼•**

```python
def _index_documents_optimized(self):
    """ä¼˜åŒ–ç‰ˆçš„ç´¢å¼•"""
    # å¹¶è¡Œç¼–ç 
    from concurrent.futures import ThreadPoolExecutor

    def encode_batch(args):
        i, batch = args
        return i, self._encode(batch)

    batches = [(i, self.documents[i:i+32])
               for i in range(0, len(self.documents), 32)]

    with ThreadPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(encode_batch, batches))

    # æ’åºå¹¶åˆå¹¶
    results.sort(key=lambda x: x[0])
    embeddings = np.vstack([r[1] for r in results])

    # æ‰¹é‡æ·»åŠ 
    keys = np.arange(len(self.documents), dtype=np.uint64)
    self.index.add(keys, embeddings)
```

**ä¼˜åŒ–2ï¼šé‡åŒ–**

```python
# ä½¿ç”¨ f16 é‡åŒ–èŠ‚çœå†…å­˜
self.index_f16 = usearch.Index(
    ndim=384,
    metric='cos',
    dtype='f16'  # åŠç²¾åº¦
)

# å†…å­˜èŠ‚çœï¼š50%
# ç²¾åº¦æŸå¤±ï¼š< 1%
```

**ä¼˜åŒ–3ï¼šå¤šé˜¶æ®µæœç´¢**

```python
def query_multi_stage(self, question, k=5):
    """ä¸¤é˜¶æ®µæœç´¢"""
    question_embedding = self._encode([question])[0]

    # é˜¶æ®µ1ï¼šç²—ç­›ï¼ˆé«˜ efï¼Œå¿«é€Ÿï¼‰
    coarse_results = self.index.search(
        question_embedding,
        k=100,
        expansion=128  # å¤§ ef æé«˜å¬å›ç‡
    )

    # é˜¶æ®µ2ï¼šé‡æ’åºï¼ˆç²¾ç¡®è®¡ç®—ï¼‰
    # ä¾‹å¦‚ï¼šä½¿ç”¨æ›´ç²¾ç¡®çš„åº¦é‡æˆ–é¢å¤–çš„ç‰¹å¾
    reranked = rerank(coarse_results, question_embedding)

    return reranked[:k]
```

---

## 2. æ¡ˆä¾‹ï¼šå›¾åƒç›¸ä¼¼åº¦æœç´¢

### 2.1 åœºæ™¯

**ä»¥å›¾æœå›¾ç³»ç»Ÿ**ï¼š

```
ä¸Šä¼ å›¾ç‰‡
    â†“
ç‰¹å¾æå–ï¼ˆResNetï¼‰â†’ 2048 ç»´å‘é‡
    â†“
USearch æœç´¢ â†’ ç›¸ä¼¼å›¾ç‰‡
    â†“
è¿”å›ç»“æœ
```

### 2.2 å®ç°

```python
import usearch
import numpy as np
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image

class ImageSearchEngine:
    def __init__(self):
        # 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.model = models.resnet50(pretrained=True)
        self.model.eval()  # æ¨ç†æ¨¡å¼

        # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
        self.feature_extractor = torch.nn.Sequential(*list(self.model.children())[:-1])

        # 2. å›¾åƒé¢„å¤„ç†
        self.preprocess = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

        # 3. åˆ›å»ºç´¢å¼•
        self.index = usearch.Index(
            ndim=2048,              # ResNet50 ç‰¹å¾ç»´åº¦
            metric='l2sq',          # L2 è·ç¦»
            dtype='f32'
        )

        self.images = []  # å­˜å‚¨å›¾åƒè·¯å¾„

    def extract_features(self, image_path):
        """æå–å›¾åƒç‰¹å¾"""
        image = Image.open(image_path).convert('RGB')
        input_tensor = self.preprocess(image)
        input_batch = input_tensor.unsqueeze(0)

        with torch.no_grad():
            features = self.feature_extractor(input_batch)

        return features.squeeze().numpy().astype(np.float32)

    def index_images(self, image_paths):
        """ç´¢å¼•å›¾åƒé›†åˆ"""
        print(f"ç´¢å¼• {len(image_paths)} å¼ å›¾ç‰‡...")

        features_list = []
        for path in image_paths:
            features = self.extract_features(path)
            features_list.append(features)
            self.images.append(path)

        features_array = np.vstack(features_list)
        keys = np.arange(len(image_paths), dtype=np.uint64)

        self.index.add(keys, features_array)
        self.index.save("image_index.usearch")

    def search(self, query_image_path, k=10):
        """æœç´¢ç›¸ä¼¼å›¾ç‰‡"""
        query_features = self.extract_features(query_image_path)
        results = self.index.search(query_features, k)

        similar_images = []
        for key, distance in results:
            image_path = self.images[key]
            similar_images.append((image_path, distance))

        return similar_images

# ä½¿ç”¨
if __name__ == "__main__":
    import glob

    engine = ImageSearchEngine()

    # ç´¢å¼•å›¾ç‰‡åº“
    image_paths = glob.glob("images/*.jpg")[:10000]
    engine.index_images(image_paths)

    # æœç´¢
    query_path = "query.jpg"
    results = engine.search(query_path, k=10)

    print("ç›¸ä¼¼å›¾ç‰‡ï¼š")
    for path, distance in results:
        print(f"  {distance:.4f}: {path}")
```

### 2.3 ä¼˜åŒ–æŠ€å·§

**æŠ€å·§1ï¼šç‰¹å¾å½’ä¸€åŒ–**

```python
def extract_features_normalized(self, image_path):
    """æå–å¹¶å½’ä¸€åŒ–ç‰¹å¾"""
    features = self.extract_features(image_path)

    # L2 å½’ä¸€åŒ–
    norm = np.linalg.norm(features)
    if norm > 0:
        features = features / norm

    return features

# å½’ä¸€åŒ–åå¯ä»¥ä½¿ç”¨ä½™å¼¦è·ç¦»ï¼Œé€šå¸¸æ›´ç¨³å®š
self.index = usearch.Index(ndim=2048, metric='cos')
```

**æŠ€å·§2ï¼šå¤šç´¢å¼•èåˆ**

```python
class MultiFeatureIndex:
    def __init__(self):
        # ç‰¹å¾1ï¼šé¢œè‰²ç›´æ–¹å›¾
        self.index_color = usearch.Index(ndim=256, metric='l2sq')

        # ç‰¹å¾2ï¼šçº¹ç†ç‰¹å¾
        self.index_texture = usearch.Index(ndim=512, metric='l2sq')

        # ç‰¹å¾3ï¼šæ·±åº¦ç‰¹å¾
        self.index_deep = usearch.Index(ndim=2048, metric='cos')

    def search(self, query_features, k=10):
        """å¤šç‰¹å¾èåˆæœç´¢"""
        # å„è‡ªæœç´¢
        results_color = self.index_color.search(query_features[0], k=k*3)
        results_texture = self.index_texture.search(query_features[1], k=k*3)
        results_deep = self.index_deep.search(query_features[2], k=k*3)

        # èåˆå¾—åˆ†
        scores = {}
        for key, dist in results_color:
            scores[key] = scores.get(key, 0) + 0.2 * (1 - dist)
        for key, dist in results_texture:
            scores[key] = scores.get(key, 0) + 0.3 * (1 - dist)
        for key, dist in results_deep:
            scores[key] = scores.get(key, 0) + 0.5 * (1 - dist)

        # æ’åº
        sorted_results = sorted(scores.items(), key=lambda x: -x[1])
        return sorted_results[:k]
```

---

## 3. æ¡ˆä¾‹ï¼šæ¨èç³»ç»Ÿ

### 3.1 åœºæ™¯

**ååŒè¿‡æ»¤ + å‘é‡æœç´¢**ï¼š

```
ç”¨æˆ·-ç‰©å“äº¤äº’çŸ©é˜µ
    â†“
çŸ©é˜µåˆ†è§£ï¼ˆMFï¼‰â†’ ç”¨æˆ·å‘é‡ã€ç‰©å“å‘é‡
    â†“
USearch æœç´¢ â†’ æ¨èç‰©å“
```

### 3.2 å®ç°

```python
import usearch
import numpy as np
from scipy.sparse.linalg import svds

class RecommenderSystem:
    def __init__(self, n_users, n_items, n_factors=64):
        self.n_users = n_users
        self.n_items = n_items
        self.n_factors = n_factors

        # ç´¢å¼•
        self.item_index = usearch.Index(
            ndim=n_factors,
            metric='ip',      # å†…ç§¯ï¼ˆæ¨èå¸¸ç”¨ï¼‰
            dtype='f32'
        )

        # çŸ©é˜µåˆ†è§£å‚æ•°
        self.user_factors = None
        self.item_factors = None

    def train(self, interaction_matrix):
        """è®­ç»ƒæ¨¡å‹"""
        print("è®­ç»ƒçŸ©é˜µåˆ†è§£æ¨¡å‹...")

        # SVD åˆ†è§£
        U, sigma, Vt = svds(interaction_matrix, k=self.n_factors)

        self.user_factors = U * sigma
        self.item_factors = Vt.T * sigma

        # ç´¢å¼•ç‰©å“
        item_keys = np.arange(self.n_items, dtype=np.uint64)
        self.item_index.add(item_keys, self.item_factors.astype(np.float32))

        print("è®­ç»ƒå®Œæˆ")

    def recommend(self, user_id, k=10, exclude_interacted=True):
        """ä¸ºç”¨æˆ·æ¨èç‰©å“"""
        # è·å–ç”¨æˆ·å‘é‡
        user_vector = self.user_factors[user_id].astype(np.float32)

        # æœç´¢
        results = self.item_index.search(user_vector, k=k*10)

        # è¿‡æ»¤å·²äº¤äº’ç‰©å“
        if exclude_interacted:
            # å‡è®¾æœ‰äº¤äº’è®°å½•
            interacted_items = get_interacted_items(user_id)
            results = [(key, score) for key, score in results
                      if key not in interacted_items]

        return results[:k]

    def similar_items(self, item_id, k=10):
        """æŸ¥æ‰¾ç›¸ä¼¼ç‰©å“"""
        item_vector = self.item_factors[item_id].astype(np.float32)
        results = self.item_index.search(item_vector, k=k+1)

        # è¿‡æ»¤è‡ªå·±
        results = [(key, score) for key, score in results if key != item_id]
        return results[:k]
```

---

## 4. æ€§èƒ½è°ƒä¼˜æŒ‡å—

### 4.1 è°ƒä¼˜æµç¨‹

```
1. åŸºå‡†æµ‹è¯•
   â†“
2. ç“¶é¢ˆåˆ†æ
   â†“
3. å‚æ•°è°ƒä¼˜
   â†“
4. æ¶æ„ä¼˜åŒ–
   â†“
5. éƒ¨ç½²ä¼˜åŒ–
```

### 4.2 å‚æ•°è°ƒä¼˜

**å‚æ•°ç½‘æ ¼æœç´¢**ï¼š

```python
import itertools

def grid_search_best_params(train_vectors, test_queries, ground_truth):
    """ç½‘æ ¼æœç´¢æœ€ä¼˜å‚æ•°"""

    # å‚æ•°ç½‘æ ¼
    connectivities = [8, 16, 32]
    expansions = [32, 64, 128]
    ef_constructions = [100, 200, 400]

    best_recall = 0
    best_params = {}

    for conn, exp, ef_const in itertools.product(
        connectivities, expansions, ef_constructions
    ):
        print(f"Testing: M={conn}, ef={exp}, ef_construction={ef_const}")

        # åˆ›å»ºç´¢å¼•
        index = usearch.Index(
            ndim=train_vectors.shape[1],
            metric='cos',
            connectivity=conn,
            expansion=exp
        )

        # è®­ç»ƒ
        keys = np.arange(len(train_vectors), dtype=np.uint64)
        index.add(keys, train_vectors)

        # æµ‹è¯•
        recalls = []
        for query, true_neighbors in zip(test_queries, ground_truth):
            results = index.search(query, k=10)
            retrieved = [key for key, _ in results]
            recall = len(set(retrieved) & set(true_neighbors)) / len(true_neighbors)
            recalls.append(recall)

        avg_recall = np.mean(recalls)

        print(f"  Recall@10: {avg_recall:.3f}")

        if avg_recall > best_recall:
            best_recall = avg_recall
            best_params = {
                'connectivity': conn,
                'expansion': exp,
                'ef_construction': ef_const
            }

    print(f"\nBest params: {best_params}")
    print(f"Best recall: {best_recall:.3f}")
    return best_params
```

### 4.3 å†…å­˜ä¼˜åŒ–

**å†…å­˜ä¼°ç®—å™¨**ï¼š

```python
def estimate_memory(n_vectors, dimensions, scalar_type='f32', M=16):
    """ä¼°ç®—å†…å­˜ä½¿ç”¨"""

    # å‘é‡å†…å­˜
    scalar_sizes = {'f64': 8, 'f32': 4, 'f16': 2, 'i8': 1}
    bytes_per_vector = dimensions * scalar_sizes[scalar_type]
    vectors_mem = n_vectors * bytes_per_vector

    # å›¾å†…å­˜ï¼ˆæ¯ä¸ªèŠ‚ç‚¹å¹³å‡ log(n_vectors) å±‚ï¼‰
    import math
    avg_levels = math.log(n_vectors, 2) * 0.25  # ç»éªŒå…¬å¼
    edges_per_node = M * avg_levels
    graph_mem = n_vectors * edges_per_node * 8  # 8 å­—èŠ‚æ¯æ¡è¾¹

    # å…ƒæ•°æ®
    metadata_mem = n_vectors * 16  # é”® + å±‚çº§

    total_mem = vectors_mem + graph_mem + metadata_mem

    print(f"å†…å­˜ä¼°ç®—ï¼ˆ{n_vectors:,} å‘é‡ï¼Œ{dimensions} ç»´ï¼‰:")
    print(f"  å‘é‡æ•°æ®: {vectors_mem/1024/1024:.1f} MB")
    print(f"  å›¾ç»“æ„: {graph_mem/1024/1024:.1f} MB")
    print(f"  å…ƒæ•°æ®: {metadata_mem/1024/1024:.1f} MB")
    print(f"  æ€»è®¡: {total_mem/1024/1024:.1f} MB")

    return total_mem

# ä½¿ç”¨
estimate_memory(1_000_000, 768, 'f32')
```

### 4.4 å¹¶è¡Œä¼˜åŒ–

**æ‰¹é‡æŸ¥è¯¢å¤„ç†**ï¼š

```python
from concurrent.futures import ThreadPoolExecutor
import numpy as np

def batch_search(index, queries, k=10, n_workers=8):
    """å¹¶è¡Œæ‰¹é‡æœç´¢"""

    def single_search(query):
        return index.search(query, k)

    with ThreadPoolExecutor(max_workers=n_workers) as executor:
        results = list(executor.map(single_search, queries))

    return results

# ä½¿ç”¨
queries = [np.random.rand(768).astype(np.float32) for _ in range(1000)]
results = batch_search(index, queries, n_workers=8)
```

---

## 5. ç”Ÿäº§éƒ¨ç½²

### 5.1 æœåŠ¡åŒ–

**REST API**ï¼š

```python
from flask import Flask, request, jsonify
import usearch
import numpy as np

app = Flask(__name__)

# å…¨å±€ç´¢å¼•
index = None

def load_index():
    global index
    index = usearch.Index.load("production_index.usearch")
    print("ç´¢å¼•åŠ è½½å®Œæˆ")

@app.route('/search', methods=['POST'])
def search():
    data = request.json
    vector = np.array(data['vector'], dtype=np.float32)
    k = data.get('k', 10)

    results = index.search(vector, k)

    return jsonify({
        'results': [{'key': int(key), 'distance': float(dist)}
                   for key, dist in results]
    })

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'ok'})

if __name__ == '__main__':
    load_index()
    app.run(host='0.0.0.0', port=8080, threaded=True)
```

### 5.2 ç›‘æ§

**æ€§èƒ½ç›‘æ§**ï¼š

```python
import time
import prometheus_client as prom

# æŒ‡æ ‡
search_latency = prom.Histogram('search_latency_seconds', 'Search latency')
search_count = prom.Counter('search_count', 'Total searches')

class MonitoredIndex:
    def __init__(self, index):
        self.index = index

    def search(self, vector, k=10):
        search_count.inc()

        with search_latency.time():
            results = self.index.search(vector, k)

        return results

# Prometheus ç«¯ç‚¹
@app.route('/metrics')
def metrics():
    return prom.generate_latest()
```

### 5.3 é«˜å¯ç”¨

**ä¸»å¤‡éƒ¨ç½²**ï¼š

```python
class HighAvailabilityIndex:
    def __init__(self, primary_path, backup_path):
        self.primary = usearch.Index.load(primary_path)
        self.backup = usearch.Index.load(backup_path)

    def search(self, vector, k=10):
        try:
            return self.primary.search(vector, k)
        except Exception as e:
            print(f"Primary failed: {e}, using backup")
            return self.backup.search(vector, k)

    def sync(self):
        """åŒæ­¥ä¸»å¤‡"""
        # å®šæœŸä¿å­˜ä¸»ç´¢å¼•
        self.primary.save("primary.usearch")

        # å¤åˆ¶åˆ°å¤‡ç´¢å¼•
        import shutil
        shutil.copy("primary.usearch", "backup.usearch")
        self.backup = usearch.Index.load("backup.usearch")
```

---

## 6. æœ€ä½³å®è·µæ€»ç»“

### 6.1 ç´¢å¼•æ„å»º

âœ… **DOï¼ˆæ¨èåšæ³•ï¼‰**ï¼š
```python
# 1. æ‰¹é‡æ·»åŠ 
index.add(keys, vectors)  # è€Œä¸æ˜¯å¾ªç¯ add

# 2. é¢„åˆ†é…
index.reserve(1_000_000)

# 3. é€‰æ‹©åˆé€‚çš„å‚æ•°
index = Index(
    ndim=768,
    metric='cos',
    connectivity=16,     # å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½
    expansion=64
)
```

âŒ **DON'Tï¼ˆé¿å…ï¼‰**ï¼š
```python
# 1. å•æ¡æ·»åŠ 
for key, vec in zip(keys, vectors):
    index.add(key, vec)  # æ…¢

# 2. ç›²ç›®å¢å¤§å‚æ•°
index = Index(connectivity=64, expansion=256)  # è¿‡å¤§
```

### 6.2 æœç´¢ä¼˜åŒ–

âœ… **DO**ï¼š
```python
# 1. è°ƒæ•´ ef
results = index.search(query, k=10, expansion=128)  # é«˜ç²¾åº¦

# 2. æ‰¹é‡æœç´¢
results = batch_search(index, queries, n_workers=8)

# 3. ä½¿ç”¨å†…å­˜æ˜ å°„
index = Index.restore("large_index.usearch", view=True)
```

âŒ **DON'T**ï¼š
```python
# 1. ef å¤ªå°
results = index.search(query, k=10, expansion=8)  # ä½å¬å›ç‡

# 2. é¢‘ç¹åˆ›å»ºç´¢å¼•
def search(query):
    index = Index.load("index.usearch")  # æ¯æ¬¡éƒ½åŠ è½½
    return index.search(query)
```

### 6.3 å†…å­˜ç®¡ç†

âœ… **DO**ï¼š
```python
# 1. ä½¿ç”¨é‡åŒ–
index = Index(dtype='f16')  # èŠ‚çœ 50% å†…å­˜

# 2. é‡Šæ”¾ä¸ç”¨çš„èµ„æº
vectors = None  # ä½¿ç”¨åé‡Šæ”¾

# 3. ç›‘æ§å†…å­˜
import psutil
print(psutil.virtual_memory())
```

### 6.4 å¹¶å‘æ§åˆ¶

âœ… **DO**ï¼š
```python
# 1. åªè¯»æœç´¢å¯ä»¥å¤šçº¿ç¨‹
with ThreadPoolExecutor(max_workers=8) as executor:
    results = executor.map(lambda q: index.search(q), queries)

# 2. å†™æ“ä½œåŠ é”
lock = threading.Lock()
with lock:
    index.add(key, vector)
```

---

## 7. è¯¾ç¨‹æ€»ç»“

### 14å¤©çŸ¥è¯†å›é¡¾

**ç¬¬1-2å¤©ï¼šåŸºç¡€**
- USearch æ¶æ„è®¾è®¡
- HNSW ç®—æ³•åŸç†

**ç¬¬3-4å¤©ï¼šæ•°æ®ç»“æ„**
- èŠ‚ç‚¹å’Œé‚»æ¥è¡¨
- å‘é‡ç´¢å¼•å®ç°

**ç¬¬5-6å¤©ï¼šæ ¸å¿ƒç®—æ³•**
- è·ç¦»è®¡ç®—ç³»ç»Ÿ
- æœç´¢ç®—æ³•è¯¦è§£

**ç¬¬7å¤©ï¼šæ’å…¥ç®—æ³•**
- å±‚çº§åˆ†é…
- é‚»å±…é€‰æ‹©å’Œå‰ªæ

**ç¬¬8å¤©ï¼šå†…å­˜ç®¡ç†**
- åŒåˆ†é…å™¨è®¾è®¡
- å†…å­˜æ˜ å°„æŠ€æœ¯

**ç¬¬9å¤©ï¼šSIMD ä¼˜åŒ–**
- ç¡¬ä»¶åŠ é€Ÿ
- SimSIMD é›†æˆ

**ç¬¬10å¤©ï¼šå¹¶å‘æ§åˆ¶**
- OpenMP å¹¶è¡ŒåŒ–
- ç»†ç²’åº¦é”

**ç¬¬11å¤©ï¼šé‡åŒ–æŠ€æœ¯**
- F16/BF16/I8
- ä¹˜ç§¯é‡åŒ–

**ç¬¬12å¤©ï¼šåºåˆ—åŒ–**
- äºŒè¿›åˆ¶æ ¼å¼è®¾è®¡
- è·¨å¹³å°å…¼å®¹

**ç¬¬13å¤©ï¼šæ€§èƒ½ä¼˜åŒ–**
- ç¼“å­˜ä¼˜åŒ–
- é¢„å–å’Œåˆ†æ”¯é¢„æµ‹

**ç¬¬14å¤©ï¼šç»¼åˆåº”ç”¨**
- RAG ç³»ç»Ÿ
- å›¾åƒæœç´¢
- æ¨èç³»ç»Ÿ

### å…³é”®è®¾è®¡åŸåˆ™

1. **ç®€æ´æ€§**ï¼šå•æ–‡ä»¶å¤´æ–‡ä»¶åº“
2. **æ€§èƒ½**ï¼šSIMD + å¹¶è¡Œ + ä¼˜åŒ–
3. **å¯ç§»æ¤æ€§**ï¼šè·¨å¹³å°å…¼å®¹
4. **çµæ´»æ€§**ï¼šå¤šè¯­è¨€ã€å¤šåº¦é‡ã€å¤šé‡åŒ–

### è¿›ä¸€æ­¥å­¦ä¹ 

**æ¨èèµ„æº**ï¼š

1. **è®ºæ–‡**ï¼š
   - "Efficient and Robust Approximate Nearest Neighbor Search" (HNSW åŸè®ºæ–‡)
   - "Product Quantization for Nearest Neighbor Search" (PQ)

2. **ä»£ç **ï¼š
   - USearch GitHub: https://github.com/unum-cloud/usearch
   - SimSIMD: https://github.com/ashvardanian/simsimd

3. **åº”ç”¨**ï¼š
   - å‘é‡æ•°æ®åº“ï¼šMilvusã€Qdrantã€Weaviate
   - åµŒå…¥æ¨¡å‹ï¼šSentence-Transformersã€Cohere

---

## 8. ç»“è¯­

æ­å–œä½ å®Œæˆäº†ä¸ºæœŸ14å¤©çš„ USearch æ·±åº¦å­¦ä¹ ä¹‹æ—…ï¼

**ä½ æŒæ¡äº†**ï¼š
- âœ… HNSW ç®—æ³•çš„å®Œæ•´å®ç°
- âœ… é«˜æ€§èƒ½ç¼–ç¨‹çš„æ ¸å¿ƒæŠ€å·§
- âœ… å‘é‡æœç´¢ç³»ç»Ÿçš„æ„å»ºæ–¹æ³•
- âœ… ç”Ÿäº§ç¯å¢ƒçš„æœ€ä½³å®è·µ

**ä¸‹ä¸€æ­¥**ï¼š
1. é˜…è¯»æºç ï¼Œæ·±å…¥ç†è§£ç»†èŠ‚
2. å®è·µé¡¹ç›®ï¼Œåº”ç”¨æ‰€å­¦çŸ¥è¯†
3. è´¡çŒ®ç¤¾åŒºï¼Œæå‡ºæ”¹è¿›å»ºè®®
4. æ¢ç´¢æ›´å¤šå‘é‡æœç´¢æŠ€æœ¯

**è®°ä½**ï¼šæ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œæ°¸è¿œæœ‰æ”¹è¿›çš„ç©ºé—´ï¼

---

## ğŸ“ è¯¾åé¡¹ç›®

**ç»¼åˆé¡¹ç›®ï¼šæ„å»ºä½ è‡ªå·±çš„å‘é‡æœç´¢å¼•æ“**

è¦æ±‚ï¼š
1. æ”¯æŒå¤šç§è·ç¦»åº¦é‡
2. å®ç° HNSW ç®—æ³•
3. æä¾›å‘½ä»¤è¡Œå’Œ Python æ¥å£
4. åŒ…å«æ€§èƒ½æµ‹è¯•å’Œå¯¹æ¯”
5. æ’°å†™æŠ€æœ¯æ–‡æ¡£

**æç¤º**ï¼šå‚è€ƒ USearch çš„è®¾è®¡ï¼Œä½†åŠ å…¥ä½ è‡ªå·±çš„åˆ›æ–°ï¼

---

**ç¬¬14å¤©å®Œæˆï¼** ğŸ‰ğŸŠğŸ“

æ„Ÿè°¢ä½ çš„åšæŒå’ŒåŠªåŠ›ï¼Œç¥ä½ åœ¨å‘é‡æœç´¢é¢†åŸŸå–å¾—æˆåŠŸï¼
